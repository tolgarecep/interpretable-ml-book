<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.3 GLM, GAM ve daha fazlası | Yorumlanabilir Makine Öğrenmesi</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="5.3 GLM, GAM and more | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.3 GLM, GAM and more | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2022-03-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logistic.html"/>
<link rel="next" href="tree.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>

<style>

#cta-button-desktop:hover, #cta-button-device:hover {
  background-color:   #ffc266; 
  border-color:   #ffc266; 
  box-shadow: none;
}
#cta-button-desktop, #cta-button-device{
  color: white;
  background-color:  #ffa31a;
  text-shadow:1px 1px 0 #444;
  text-decoration: none;
  border: 2px solid  #ffa31a;
  border-radius: 10px;
  position: fixed;
  padding: 5px 10px;
  z-index: 10;
  }

#cta-button-device {
  box-shadow: 0px 10px 10px -5px rgba(194,180,190,1);
  display:none;
  right: 20px;
  bottom: 20px;
  font-size: 20px;
 }

#cta-button-desktop {
  box-shadow: 0px 20px 20px -10px rgba(194,180,190,1);
  display:display;
  padding: 8px 16px;
  right: 40px;
  bottom: 40px;
  font-size: 25px;
}

@media (max-width : 450px) {
  #cta-button-device {display:block;}
  #cta-button-desktop {display:none;}
}


</style>


<a id="cta-button-desktop" href="https://leanpub.com/interpretable-machine-learning" rel="noopener noreferrer" target="blank"> Buy Book </a>

<a id="cta-button-device" href="https://leanpub.com/interpretable-machine-learning" rel="noopener noreferrer" target="blank">Buy</a>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">
  <!--Interpretable machine learning-->
  Yorumlanabilir Makine Öğrenmesi
  </a></li>

<li class="divider"></li>
<li><a href="index.html#summary"><!--Summary-->Özet<span></span></a></li>
<li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="preface-by-the-author.html"><i class="fa fa-check"></i><b>1</b>
  <!--Preface by the Author-->Yazarın Önsözü<span></span></a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b><!--Introduction--> Giriş<span></span></a><ul>
<li class="chapter" data-level="2.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>2.1</b>
  <!--Story Time-->Hikaye Vakti<span></span></a><ul>
<li><a href="storytime.html#lightning-never-strikes-twice"><!--Lightning Never Strikes Twice-->Yıldırım Aynı Yere İki Kez Düşmez<span></span></a></li>
<li><a href="storytime.html#trust-fall">Trust Fall<span></span></a></li>
<li><a href="storytime.html#fermis-paperclips">Fermi’s Paperclips<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2.2</b>
  <!--What Is Machine Learning?--> Makine Öğrenmesi Nedir?<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>2.3</b>
  <!--Terminology-->Terminoloji<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b>
  <!--Interpretability-->Yorumlanabilirlik<span></span></a><ul>
<li class="chapter" data-level="3.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>3.1</b>
  <!--Importance of Interpretability--> Yorumlanabilirliğin Önemi<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="taxonomy-of-interpretability-methods.html"><a href="taxonomy-of-interpretability-methods.html"><i class="fa fa-check"></i><b>3.2</b>
  <!--Taxonomy of Interpretability Methods--> Yorumlama Metotlarının Sınıflandırılması<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>3.3</b>
  <!--Scope of Interpretability--> Yorumlanabilirliğin Kapsamı<span></span></a><ul>
<li class="chapter" data-level="3.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>3.3.1</b>
  <!--Algorithm Transparency--> Algoritmaların Şeffaflığı<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>3.3.2</b>
  Global, Holistic Model Interpretability<span></span></a></li>
<li class="chapter" data-level="3.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>3.3.3</b>
  Global Model Interpretability on a Modular Level<span></span></a></li>
<li class="chapter" data-level="3.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>3.3.4</b>
  Local Interpretability for a Single Prediction<span></span></a></li>
<li class="chapter" data-level="3.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-predictions"><i class="fa fa-check"></i><b>3.3.5</b>
  Local Interpretability for a Group of Predictions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="evaluation-of-interpretability.html"><a href="evaluation-of-interpretability.html"><i class="fa fa-check"></i><b>3.4</b>
  <!--Evaluation of Interpretability--> Yorumlanabilirliğin Değerlendirilmesi<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>3.5</b>
  <!--Properties of Explanations--> Açıklamaların Özellikleri<span></span></a></li>
<li class="chapter" data-level="3.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>3.6</b>
  <!--Human-friendly Explanations--> İnsan Dostu Açıklamalar<span></span></a><ul>
<li class="chapter" data-level="3.6.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>3.6.1</b>
  <!--What Is an Explanation?--> Açıklama Nedir?<span></span></a></li>
<li class="chapter" data-level="3.6.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>3.6.2</b>
  <!--What Is a Good Explanation?-->İyi Açıklama Nasıl Olur?<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b><!--Datasets--> Veriler<span></span></a><ul>
<li class="chapter" data-level="4.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>4.1</b>
  <!--Bike Rentals (Regression)--> Bisiklet Kiralama (Regression)<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>4.2</b>
  <!--YouTube Spam Comments (Text Classification)--> Youtube Spam Yorumlar (Metin Sınıflandırması)<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>4.3</b>
  <!--Risk Factors for Cervical Cancer (Classification)--> Rahim Ağzı Kanseri (Sınıflandırma)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> 
  <!--Interpretable Models-->Yorumlanabilir Modeller<span></span></a><ul>
<li class="chapter" data-level="5.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>5.1</b> 
  Linear Regression<span></span></a><ul>
<li class="chapter" data-level="5.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>5.1.1</b> 
  <!--Interpretation--> Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>5.1.2</b>
  <!--Example--> Örnek<span></span></a></li>
<li class="chapter" data-level="5.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>5.1.3</b>
  <!--Visual Interpretation--> Görsel Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.1.4" data-path="limo.html"><a href="limo.html#explain-individual-predictions"><i class="fa fa-check"></i><b>5.1.4</b>
  <!--Explain Individual Predictions-->Özel Tahminleri Açıklama<span></span></a></li>
<li class="chapter" data-level="5.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>5.1.5</b>
  <!--Encoding of Categorical Features--> Kategorik Niteliklerin Şifrelenmesi<span></span></a></li>
<li class="chapter" data-level="5.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>5.1.6</b>
  <!--Do Linear Models Create Good Explanations?--> Lineer Modeller İyi Açıklamalar Meydana Getirir Mi?<span></span></a></li>
<li class="chapter" data-level="5.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>5.1.7</b> 
  <!--Sparse Linear Models--> Aralıklı Lineer Modeller<span></span></a></li>
<li class="chapter" data-level="5.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>5.1.8</b> 
  <!--Advantages--> Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>5.1.9</b>
  <!--Disadvantages--> Dezavantajlar<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression<span></span></a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic.html"><a href="logistic.html#what-is-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>5.2.1</b>
  <!--What is Wrong with Linear Regression for Classification?--> Linear Regression ile Sınıflandırmanın Problemi Ne?<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic.html"><a href="logistic.html#theory"><i class="fa fa-check"></i><b>5.2.2</b>
  <!--Theory--> Teori<span></span></a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>5.2.3</b>
  <!--Interpretation--> Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.2.4" data-path="logistic.html"><a href="logistic.html#example-1"><i class="fa fa-check"></i><b>5.2.4</b>
  <!--Example--> Örnek<span></span></a></li>
<li class="chapter" data-level="5.2.5" data-path="logistic.html"><a href="logistic.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>5.2.5</b>
  <!--Advantages and Disadvantages--> Avantajlar ve Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.2.6" data-path="logistic.html"><a href="logistic.html#software"><i class="fa fa-check"></i><b>5.2.6</b>
  <!--Software--> Yazılım<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>5.3</b> 
  <!--GLM, GAM and more-->GLM, GAM ve daha fazlası<span></span></a><ul>
<li class="chapter" data-level="5.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>5.3.1</b>
  <!--Non-Gaussian Outcomes - GLMs--> Non-Gaussian Sonuçlar - GLM'ler<span></span></a></li>
<li class="chapter" data-level="5.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>5.3.2</b>
  <!--Interactions--> Etkileşimler<span></span></a></li>
<li class="chapter" data-level="5.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>5.3.3</b>
  <!--Nonlinear Effects - GAMs--> Non-lineer etkiler - GAM'lar<span></span></a></li>
<li class="chapter" data-level="5.3.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>5.3.4</b>
  <!--Advantages--> Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.3.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>5.3.5</b>
  <!--Disadvantages--> Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.3.6" data-path="extend-lm.html"><a href="extend-lm.html#software-1"><i class="fa fa-check"></i><b>5.3.6</b>
  <!--Software--> Yazılım<span></span></a></li>
<li class="chapter" data-level="5.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>5.3.7</b>
  <!--Further Extensions--> İlaveler<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>5.4</b>
  <!--Decision Tree--> Karar Ağaçları<span></span></a><ul>
<li class="chapter" data-level="5.4.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>5.4.1</b>
  <!--Interpretation--> Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.4.2" data-path="tree.html"><a href="tree.html#example-2"><i class="fa fa-check"></i><b>5.4.2</b>
  <!--Example--> Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.4.3" data-path="tree.html"><a href="tree.html#advantages-2"><i class="fa fa-check"></i><b>5.4.3</b>
  <!--Advantages--> Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.4.4" data-path="tree.html"><a href="tree.html#disadvantages-2"><i class="fa fa-check"></i><b>5.4.4</b>
  <!--Disadvantages--> Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.4.5" data-path="tree.html"><a href="tree.html#software-2"><i class="fa fa-check"></i><b>5.4.5</b>
  <!--Software--> Yazılım<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>5.5</b>
  <!--Decision Rules--> Karar Kuralları<span></span></a><ul>
<li class="chapter" data-level="5.5.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>5.5.1</b>
  <!--Learn Rules from a Single Feature (OneR)--> Yalnızca Bir Nitelikten Kural Öğrenme (OneR)<span></span></a></li>
<li class="chapter" data-level="5.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>5.5.2</b> Sequential Covering<span></span></a></li>
<li class="chapter" data-level="5.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>5.5.3</b> 
  <!--Bayesian Rule Lists-->Bayes-yan Kural Listeleri<span></span></a></li>
<li class="chapter" data-level="5.5.4" data-path="rules.html"><a href="rules.html#advantages-3"><i class="fa fa-check"></i><b>5.5.4</b>
  <!--Advantages--> Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.5.5" data-path="rules.html"><a href="rules.html#disadvantages-3"><i class="fa fa-check"></i><b>5.5.5</b>
  <!--Disadvantages--> Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.5.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>5.5.6</b>
  <!--Software and Alternatives--> Yazılım ve alternatifler<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>5.6</b> RuleFit<span></span></a><ul>
<li class="chapter" data-level="5.6.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>5.6.1</b>
  <!--Interpretation and Example--> Yorumlama ve Örnek<span></span></a></li>
<li class="chapter" data-level="5.6.2" data-path="rulefit.html"><a href="rulefit.html#theory-1"><i class="fa fa-check"></i><b>5.6.2</b>
  <!--Theory--> Teori<span></span></a></li>
<li class="chapter" data-level="5.6.3" data-path="rulefit.html"><a href="rulefit.html#advantages-4"><i class="fa fa-check"></i><b>5.6.3</b> 
  <!--Advantages-->Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.6.4" data-path="rulefit.html"><a href="rulefit.html#disadvantages-4"><i class="fa fa-check"></i><b>5.6.4</b>
  <!--Disadvantages-->Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.6.5" data-path="rulefit.html"><a href="rulefit.html#software-and-alternative"><i class="fa fa-check"></i><b>5.6.5</b>
  <!--Software and Alternative-->Yazıım ve alternatif<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>5.7</b>
  <!--Other Interpretable Models--> Diğer Yorumlanabilir Modeller<span></span></a><ul>
<li class="chapter" data-level="5.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>5.7.1</b>
  <!--Naive Bayes Classifier--> Naive Bayes ile Sınıflandırma<span></span></a></li>
<li class="chapter" data-level="5.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.7.2</b> K-Nearest Neighbors<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>6</b> 
  <!--Model-Agnostic Methods-->Modelden Bağımsız Metotlar<span></span></a></li>
<li class="chapter" data-level="7" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>7</b> 
  <!--Example-Based Explanations-->Örneklere Dayalı Açıklamalar<span></span></a></li>
<li class="chapter" data-level="8" data-path="global-methods.html"><a href="global-methods.html"><i class="fa fa-check"></i><b>8</b> 
  <!--Global Model-Agnostic Methods-->Modelden Bağımsız Evrensel Metotlar<span></span></a><ul>
<li class="chapter" data-level="8.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>8.1</b> 
  <!--Partial Dependence Plot (PDP)-->Kısmi Bağımlılık Grafiği (PDP)<span></span></a><ul>
<li class="chapter" data-level="8.1.1" data-path="pdp.html"><a href="pdp.html#pdp-based-feature-importance"><i class="fa fa-check"></i><b>8.1.1</b> 
  <!--PDP-based Feature Importance-->PDP Tabanlı Nitelik Değerleri<span></span></a></li>
<li class="chapter" data-level="8.1.2" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>8.1.2</b> Examples<span></span></a></li>
<li class="chapter" data-level="8.1.3" data-path="pdp.html"><a href="pdp.html#advantages-5"><i class="fa fa-check"></i><b>8.1.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.1.4" data-path="pdp.html"><a href="pdp.html#disadvantages-5"><i class="fa fa-check"></i><b>8.1.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.1.5" data-path="pdp.html"><a href="pdp.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>8.1.5</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>8.2</b> Accumulated Local Effects (ALE) Plot<span></span></a><ul>
<li class="chapter" data-level="8.2.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>8.2.1</b> Motivation and Intuition<span></span></a></li>
<li class="chapter" data-level="8.2.2" data-path="ale.html"><a href="ale.html#theory-2"><i class="fa fa-check"></i><b>8.2.2</b> Theory<span></span></a></li>
<li class="chapter" data-level="8.2.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>8.2.3</b> Estimation<span></span></a></li>
<li class="chapter" data-level="8.2.4" data-path="ale.html"><a href="ale.html#examples-1"><i class="fa fa-check"></i><b>8.2.4</b> Examples<span></span></a></li>
<li class="chapter" data-level="8.2.5" data-path="ale.html"><a href="ale.html#advantages-6"><i class="fa fa-check"></i><b>8.2.5</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.2.6" data-path="ale.html"><a href="ale.html#disadvantages-6"><i class="fa fa-check"></i><b>8.2.6</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.2.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>8.2.7</b> Implementation and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>8.3</b> Feature Interaction<span></span></a><ul>
<li class="chapter" data-level="8.3.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>8.3.1</b> Feature Interaction?<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>8.3.2</b> Theory: Friedman’s H-statistic<span></span></a></li>
<li class="chapter" data-level="8.3.3" data-path="interaction.html"><a href="interaction.html#examples-2"><i class="fa fa-check"></i><b>8.3.3</b> Examples<span></span></a></li>
<li class="chapter" data-level="8.3.4" data-path="interaction.html"><a href="interaction.html#advantages-7"><i class="fa fa-check"></i><b>8.3.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.3.5" data-path="interaction.html"><a href="interaction.html#disadvantages-7"><i class="fa fa-check"></i><b>8.3.5</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.3.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>8.3.6</b> Implementations<span></span></a></li>
<li class="chapter" data-level="8.3.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>8.3.7</b> Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>8.4</b> Functional Decompositon<span></span></a><ul>
<li class="chapter" data-level="8.4.1" data-path="decomposition.html"><a href="decomposition.html#how-not-to-compute-the-components-i"><i class="fa fa-check"></i><b>8.4.1</b> How not to Compute the Components I<span></span></a></li>
<li class="chapter" data-level="8.4.2" data-path="decomposition.html"><a href="decomposition.html#functional-decomposition"><i class="fa fa-check"></i><b>8.4.2</b> Functional Decomposition<span></span></a></li>
<li class="chapter" data-level="8.4.3" data-path="decomposition.html"><a href="decomposition.html#how-not-to-compute-the-components-ii"><i class="fa fa-check"></i><b>8.4.3</b> How not to Compute the Components II<span></span></a></li>
<li class="chapter" data-level="8.4.4" data-path="decomposition.html"><a href="decomposition.html#functional-anova"><i class="fa fa-check"></i><b>8.4.4</b> Functional ANOVA<span></span></a></li>
<li class="chapter" data-level="8.4.5" data-path="decomposition.html"><a href="decomposition.html#generalized-functional-anova-for-dependent-features"><i class="fa fa-check"></i><b>8.4.5</b> Generalized Functional ANOVA for Dependent Features<span></span></a></li>
<li class="chapter" data-level="8.4.6" data-path="decomposition.html"><a href="decomposition.html#accumulated-local-effect-plots"><i class="fa fa-check"></i><b>8.4.6</b> Accumulated Local Effect Plots<span></span></a></li>
<li class="chapter" data-level="8.4.7" data-path="decomposition.html"><a href="decomposition.html#statistical-regression-models"><i class="fa fa-check"></i><b>8.4.7</b> Statistical Regression Models<span></span></a></li>
<li class="chapter" data-level="8.4.8" data-path="decomposition.html"><a href="decomposition.html#bonus-partial-dependence-plot"><i class="fa fa-check"></i><b>8.4.8</b> Bonus: Partial Dependence Plot<span></span></a></li>
<li class="chapter" data-level="8.4.9" data-path="decomposition.html"><a href="decomposition.html#advantages-8"><i class="fa fa-check"></i><b>8.4.9</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.4.10" data-path="decomposition.html"><a href="decomposition.html#disadvantages-8"><i class="fa fa-check"></i><b>8.4.10</b> Disadvantages<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>8.5</b> Permutation Feature Importance<span></span></a><ul>
<li class="chapter" data-level="8.5.1" data-path="feature-importance.html"><a href="feature-importance.html#theory-3"><i class="fa fa-check"></i><b>8.5.1</b> Theory<span></span></a></li>
<li class="chapter" data-level="8.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>8.5.2</b> Should I Compute Importance on Training or Test Data?<span></span></a></li>
<li class="chapter" data-level="8.5.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>8.5.3</b> Example and Interpretation<span></span></a></li>
<li class="chapter" data-level="8.5.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-9"><i class="fa fa-check"></i><b>8.5.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.5.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-9"><i class="fa fa-check"></i><b>8.5.5</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.5.6" data-path="feature-importance.html"><a href="feature-importance.html#alternatives-1"><i class="fa fa-check"></i><b>8.5.6</b> Alternatives<span></span></a></li>
<li class="chapter" data-level="8.5.7" data-path="feature-importance.html"><a href="feature-importance.html#software-3"><i class="fa fa-check"></i><b>8.5.7</b> Software<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>8.6</b> Global Surrogate<span></span></a><ul>
<li class="chapter" data-level="8.6.1" data-path="global.html"><a href="global.html#theory-4"><i class="fa fa-check"></i><b>8.6.1</b> Theory<span></span></a></li>
<li class="chapter" data-level="8.6.2" data-path="global.html"><a href="global.html#example-3"><i class="fa fa-check"></i><b>8.6.2</b> Example<span></span></a></li>
<li class="chapter" data-level="8.6.3" data-path="global.html"><a href="global.html#advantages-10"><i class="fa fa-check"></i><b>8.6.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.6.4" data-path="global.html"><a href="global.html#disadvantages-10"><i class="fa fa-check"></i><b>8.6.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.6.5" data-path="global.html"><a href="global.html#software-4"><i class="fa fa-check"></i><b>8.6.5</b> Software<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>8.7</b> Prototypes and Criticisms<span></span></a><ul>
<li class="chapter" data-level="8.7.1" data-path="proto.html"><a href="proto.html#theory-5"><i class="fa fa-check"></i><b>8.7.1</b> Theory<span></span></a></li>
<li class="chapter" data-level="8.7.2" data-path="proto.html"><a href="proto.html#examples-3"><i class="fa fa-check"></i><b>8.7.2</b> Examples<span></span></a></li>
<li class="chapter" data-level="8.7.3" data-path="proto.html"><a href="proto.html#advantages-11"><i class="fa fa-check"></i><b>8.7.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.7.4" data-path="proto.html"><a href="proto.html#disadvantages-11"><i class="fa fa-check"></i><b>8.7.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.7.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>8.7.5</b> Code and Alternatives<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="local-methods.html"><a href="local-methods.html"><i class="fa fa-check"></i><b>9</b> 
  <!--Local Model-Agnostic Methods-->Modelden Bağımsız Yerel Metotlar<span></span></a><ul>
<li class="chapter" data-level="9.1" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>9.1</b> Individual Conditional Expectation (ICE)<span></span></a><ul>
<li class="chapter" data-level="9.1.1" data-path="ice.html"><a href="ice.html#examples-4"><i class="fa fa-check"></i><b>9.1.1</b> Examples<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="ice.html"><a href="ice.html#advantages-12"><i class="fa fa-check"></i><b>9.1.2</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.1.3" data-path="ice.html"><a href="ice.html#disadvantages-12"><i class="fa fa-check"></i><b>9.1.3</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.1.4" data-path="ice.html"><a href="ice.html#software-and-alternatives-2"><i class="fa fa-check"></i><b>9.1.4</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>9.2</b> Local Surrogate (LIME)<span></span></a><ul>
<li class="chapter" data-level="9.2.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>9.2.1</b> LIME for Tabular Data<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>9.2.2</b> LIME for Text<span></span></a></li>
<li class="chapter" data-level="9.2.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>9.2.3</b> LIME for Images<span></span></a></li>
<li class="chapter" data-level="9.2.4" data-path="lime.html"><a href="lime.html#advantages-13"><i class="fa fa-check"></i><b>9.2.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.2.5" data-path="lime.html"><a href="lime.html#disadvantages-13"><i class="fa fa-check"></i><b>9.2.5</b> Disadvantages<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>9.3</b> Counterfactual Explanations<span></span></a><ul>
<li class="chapter" data-level="9.3.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>9.3.1</b> Generating Counterfactual Explanations<span></span></a></li>
<li class="chapter" data-level="9.3.2" data-path="counterfactual.html"><a href="counterfactual.html#example-8"><i class="fa fa-check"></i><b>9.3.2</b> Example<span></span></a></li>
<li class="chapter" data-level="9.3.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-14"><i class="fa fa-check"></i><b>9.3.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.3.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-14"><i class="fa fa-check"></i><b>9.3.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.3.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>9.3.5</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>9.4</b> Scoped Rules (Anchors)<span></span></a><ul>
<li class="chapter" data-level="9.4.1" data-path="anchors.html"><a href="anchors.html#finding-anchors"><i class="fa fa-check"></i><b>9.4.1</b> Finding Anchors<span></span></a></li>
<li class="chapter" data-level="9.4.2" data-path="anchors.html"><a href="anchors.html#complexity-and-runtime"><i class="fa fa-check"></i><b>9.4.2</b> Complexity and Runtime<span></span></a></li>
<li class="chapter" data-level="9.4.3" data-path="anchors.html"><a href="anchors.html#tabular-data-example"><i class="fa fa-check"></i><b>9.4.3</b> Tabular Data Example<span></span></a></li>
<li class="chapter" data-level="9.4.4" data-path="anchors.html"><a href="anchors.html#advantages-15"><i class="fa fa-check"></i><b>9.4.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.4.5" data-path="anchors.html"><a href="anchors.html#disadvantages-15"><i class="fa fa-check"></i><b>9.4.5</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.4.6" data-path="anchors.html"><a href="anchors.html#software-and-alternatives-3"><i class="fa fa-check"></i><b>9.4.6</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>9.5</b> Shapley Values<span></span></a><ul>
<li class="chapter" data-level="9.5.1" data-path="shapley.html"><a href="shapley.html#general-idea"><i class="fa fa-check"></i><b>9.5.1</b> General Idea<span></span></a></li>
<li class="chapter" data-level="9.5.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>9.5.2</b> Examples and Interpretation<span></span></a></li>
<li class="chapter" data-level="9.5.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>9.5.3</b> The Shapley Value in Detail<span></span></a></li>
<li class="chapter" data-level="9.5.4" data-path="shapley.html"><a href="shapley.html#advantages-16"><i class="fa fa-check"></i><b>9.5.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.5.5" data-path="shapley.html"><a href="shapley.html#disadvantages-16"><i class="fa fa-check"></i><b>9.5.5</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.5.6" data-path="shapley.html"><a href="shapley.html#software-and-alternatives-4"><i class="fa fa-check"></i><b>9.5.6</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>9.6</b> SHAP (SHapley Additive exPlanations)<span></span></a><ul>
<li class="chapter" data-level="9.6.1" data-path="shap.html"><a href="shap.html#definition"><i class="fa fa-check"></i><b>9.6.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="9.6.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>9.6.2</b> KernelSHAP<span></span></a></li>
<li class="chapter" data-level="9.6.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>9.6.3</b> TreeSHAP<span></span></a></li>
<li class="chapter" data-level="9.6.4" data-path="shap.html"><a href="shap.html#examples-5"><i class="fa fa-check"></i><b>9.6.4</b> Examples<span></span></a></li>
<li class="chapter" data-level="9.6.5" data-path="shap.html"><a href="shap.html#shap-feature-importance"><i class="fa fa-check"></i><b>9.6.5</b> SHAP Feature Importance<span></span></a></li>
<li class="chapter" data-level="9.6.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>9.6.6</b> SHAP Summary Plot<span></span></a></li>
<li class="chapter" data-level="9.6.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>9.6.7</b> SHAP Dependence Plot<span></span></a></li>
<li class="chapter" data-level="9.6.8" data-path="shap.html"><a href="shap.html#shap-interaction-values"><i class="fa fa-check"></i><b>9.6.8</b> SHAP Interaction Values<span></span></a></li>
<li class="chapter" data-level="9.6.9" data-path="shap.html"><a href="shap.html#clustering-shapley-values"><i class="fa fa-check"></i><b>9.6.9</b> Clustering Shapley Values<span></span></a></li>
<li class="chapter" data-level="9.6.10" data-path="shap.html"><a href="shap.html#advantages-17"><i class="fa fa-check"></i><b>9.6.10</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.6.11" data-path="shap.html"><a href="shap.html#disadvantages-17"><i class="fa fa-check"></i><b>9.6.11</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.6.12" data-path="shap.html"><a href="shap.html#software-5"><i class="fa fa-check"></i><b>9.6.12</b> Software<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>10</b> Neural Network Interpretation<span></span></a><ul>
<li class="chapter" data-level="10.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>10.1</b> Learned Features<span></span></a><ul>
<li class="chapter" data-level="10.1.1" data-path="cnn-features.html"><a href="cnn-features.html#feature-visualization"><i class="fa fa-check"></i><b>10.1.1</b> Feature Visualization<span></span></a></li>
<li class="chapter" data-level="10.1.2" data-path="cnn-features.html"><a href="cnn-features.html#network-dissection"><i class="fa fa-check"></i><b>10.1.2</b> Network Dissection<span></span></a></li>
<li class="chapter" data-level="10.1.3" data-path="cnn-features.html"><a href="cnn-features.html#advantages-18"><i class="fa fa-check"></i><b>10.1.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="10.1.4" data-path="cnn-features.html"><a href="cnn-features.html#disadvantages-18"><i class="fa fa-check"></i><b>10.1.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="10.1.5" data-path="cnn-features.html"><a href="cnn-features.html#software-and-further-material"><i class="fa fa-check"></i><b>10.1.5</b> Software and Further Material<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html"><i class="fa fa-check"></i><b>10.2</b> Pixel Attribution (Saliency Maps)<span></span></a><ul>
<li class="chapter" data-level="10.2.1" data-path="pixel-attribution.html"><a href="pixel-attribution.html#vanilla-gradient-saliency-maps"><i class="fa fa-check"></i><b>10.2.1</b> Vanilla Gradient (Saliency Maps)<span></span></a></li>
<li class="chapter" data-level="10.2.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html#deconvnet"><i class="fa fa-check"></i><b>10.2.2</b> DeconvNet<span></span></a></li>
<li class="chapter" data-level="10.2.3" data-path="pixel-attribution.html"><a href="pixel-attribution.html#grad-cam"><i class="fa fa-check"></i><b>10.2.3</b> Grad-CAM<span></span></a></li>
<li class="chapter" data-level="10.2.4" data-path="pixel-attribution.html"><a href="pixel-attribution.html#guided-grad-cam"><i class="fa fa-check"></i><b>10.2.4</b> Guided Grad-CAM<span></span></a></li>
<li class="chapter" data-level="10.2.5" data-path="pixel-attribution.html"><a href="pixel-attribution.html#smoothgrad"><i class="fa fa-check"></i><b>10.2.5</b> SmoothGrad<span></span></a></li>
<li class="chapter" data-level="10.2.6" data-path="pixel-attribution.html"><a href="pixel-attribution.html#examples-6"><i class="fa fa-check"></i><b>10.2.6</b> Examples<span></span></a></li>
<li class="chapter" data-level="10.2.7" data-path="pixel-attribution.html"><a href="pixel-attribution.html#advantages-19"><i class="fa fa-check"></i><b>10.2.7</b> Advantages<span></span></a></li>
<li class="chapter" data-level="10.2.8" data-path="pixel-attribution.html"><a href="pixel-attribution.html#disadvantages-19"><i class="fa fa-check"></i><b>10.2.8</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="10.2.9" data-path="pixel-attribution.html"><a href="pixel-attribution.html#software-6"><i class="fa fa-check"></i><b>10.2.9</b> Software<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html"><i class="fa fa-check"></i><b>10.3</b> Detecting Concepts<span></span></a><ul>
<li class="chapter" data-level="10.3.1" data-path="detecting-concepts.html"><a href="detecting-concepts.html#tcav-testing-with-concept-activation-vectors"><i class="fa fa-check"></i><b>10.3.1</b> TCAV: Testing with Concept Activation Vectors<span></span></a></li>
<li class="chapter" data-level="10.3.2" data-path="detecting-concepts.html"><a href="detecting-concepts.html#example-9"><i class="fa fa-check"></i><b>10.3.2</b> Example<span></span></a></li>
<li class="chapter" data-level="10.3.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html#advantages-20"><i class="fa fa-check"></i><b>10.3.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="10.3.4" data-path="detecting-concepts.html"><a href="detecting-concepts.html#disadvantages-20"><i class="fa fa-check"></i><b>10.3.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="10.3.5" data-path="detecting-concepts.html"><a href="detecting-concepts.html#bonus-other-concept-based-approaches"><i class="fa fa-check"></i><b>10.3.5</b> Bonus: Other Concept-based Approaches<span></span></a></li>
<li class="chapter" data-level="10.3.6" data-path="detecting-concepts.html"><a href="detecting-concepts.html#software-7"><i class="fa fa-check"></i><b>10.3.6</b> Software<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>10.4</b> Adversarial Examples<span></span></a><ul>
<li class="chapter" data-level="10.4.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>10.4.1</b> Methods and Examples<span></span></a></li>
<li class="chapter" data-level="10.4.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>10.4.2</b> The Cybersecurity Perspective<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>10.5</b> Influential Instances<span></span></a><ul>
<li class="chapter" data-level="10.5.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>10.5.1</b> Deletion Diagnostics<span></span></a></li>
<li class="chapter" data-level="10.5.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>10.5.2</b> Influence Functions<span></span></a></li>
<li class="chapter" data-level="10.5.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>10.5.3</b> Advantages of Identifying Influential Instances<span></span></a></li>
<li class="chapter" data-level="10.5.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>10.5.4</b> Disadvantages of Identifying Influential Instances<span></span></a></li>
<li class="chapter" data-level="10.5.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-5"><i class="fa fa-check"></i><b>10.5.5</b> Software and Alternatives<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>11</b> A Look into the Crystal Ball<span></span></a><ul>
<li class="chapter" data-level="11.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>11.1</b> The Future of Machine Learning<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>11.2</b> The Future of Interpretability<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>12</b> Contribute to the Book<span></span></a></li>
<li class="chapter" data-level="13" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>13</b> Citing this Book<span></span></a></li>
<li class="chapter" data-level="14" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>14</b> Translations<span></span></a></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements<span></span></a></li>
<li><a href="references.html#references">References<span></span></a><ul>
<li><a href="r-packages-used.html#r-packages-used">R Packages Used<span></span></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li> 
<li><a href="https://christophmolnar.com/impressum/" target="_blank">Impressum</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="extend-lm" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.3</span>
  GLM, GAM ve daha fazlası<a href="extend-lm.html#extend-lm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="limo.html#limo">Lineer regression</a> modellerinin en güçlü ve aynı zamanda en zayıf yanı, tahminin, özniteliklerin ağırlıklandırılmış bir toplamı
  olarak modellenmesidir. Buna ek olarak lineer modeller birçok varsayımı beraberinde getirir. Kötü haber, bu varsayımların gerçeklikte
  sık sık ihlal ediliyor olmasıdır: Tahmin etmek istediğimiz hedef öznitelik normal olmayan (non-Gaussian) bir dağılıma sahip olabilir, öznitelikler birbirleriyle
  etkileşim halinde olabilir veya öznitelikler ve hedef arasındaki ilişki lineer olmayabilir. İyi haber, istatistik topluluğunun lineer regression modelini sıradan bir bıçaktan
  bir İsveç Çakısı'na dönüştüren modifikasyonlar geliştirmiş olmasıdır.</p>
<p>Bu bölüm genişletilmiş lineer modelleri tümüyle kapsayan bir rehber değildir, onun yerine Genelleştirilmiş Lineer Modellere (GLM'lere) ve Genelleştirilmiş
  Arttırmalı Modellere (GAM'lara) genel bakış niteliğindedir ve size konu hakkında bir önsezi sağlar. Bölümü tamamladıktan sonra lineer modelleri nasıl
  genişleteceğinize dair sağlam bir bakış açısına sahip olacaksınız. Eğer önce lineer regression hakkında daha fazla şey öğrenmek isterseniz ve
  <a href="limo.html#limo">lineer regression üzerine yazdığım bölümü</a> okumadıysanız önce o bölümü okumanızı öneririm.</p>
<p>Lineer regression modelinin formülünü hatırlayarak başlayalım:</p>
  <p><span class="math display">\[y=\beta_{0}+\beta_{1}x_{1}+\ldots+\beta_{p}x_{p}+\epsilon\]</span></p>
<p>Lineer regression modeli, bir örneğin y değerinin, normal dağılım gösteren
  bir <span class="math inline">\(\epsilon\)</span> değeriyle birlikte onun p tane özniteliğinin değerlerinin ağırlıklandırılmış
  toplamı olarak ifade edilebileceğini varsayar. Veriyi bu formüle sıkıştırarak gayet yorumlanabilir bir model elde etmiş oluruz. Öznitelik etkileri
  eklemelidir, yani aralarında hiçbir etkileşim yoktur, ve ilişki lineerdir, yani herhangi bir özniteliğin bir birim arttırılması hesaplanan hedefte
  direkt bir artmaya veya azalmaya sebep olur. Lineer modeller, bir öznitelik ve hedef arasındaki ilişkiyi tek bir sayıya, yani
  o özniteliğin ağırlığına, sıkıştırmamıza yardımcı olur.</p>

<p>Fakat tek bir ağırlıklandırılmış toplam gerçek hayatta karşılaşılan çoğu problem için fazla kısıtlayıcıdır. Bu bölümde klasik lineer regression modelinin
  yol açtığı üç sorunu ve onları nasıl çözebileceğimizi öğreneceğiz. Varsayımların ihlal edilmesiyle oluşan birçok sorunla karşılaşılabilir fakat bu bölümde
  yalnızca aşağıdaki üç soruna odaklanacağız:</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:three-lm-problems"></span>
<img src="images/three-lm-problems-1.png" alt="Three assumptions of the linear model (left side): Gaussian distribution of the outcome given the features, additivity (= no interactions) and linear relationship. Reality usually does not adhere to those assumptions (right side): Outcomes might have non-Gaussian distributions, features might interact and the relationship might be nonlinear." width="\textwidth" />
<p class="caption">
ŞEKİL 5.8:
  Lineer modellerin üç varsayımı (solda):
  Hedef özniteliğin normal dağılması, eklemelilik (= etkileşim yok) ve lineer ilişki.
  Gerçeklik genelde bu varsayımlara uymaz (sağda):
  Hedefler non-Gaussian dağılmış olabilir, öznitelikler birbirleriyle etkileşimde olabilir ve ilişki non-lineer olabilir.
</p>
</div>
<p>Her üç problemin de çözümü mevcuttur:</p>
<p><strong>Problem</strong>: Hedef değişken y Gaussian bir dağılıma sahip değil.<br />
<strong>Örnek</strong>: Diyelim ki herhangi bir günde ne kadar bisiklet süreceğimi tahmin etmek istiyorum. Elimdeki öznitelikler gün ve havayla ilgili. 
  Eğer lineer bir model kullanırsam negatif dakikalar tahmin edebilir çünkü varsaydığı normal dağılım 0 dakikayla sınırlı değildir. Aynı zamanda
  eğer olasılıkları lineer bir modelle hesaplamak istersem negatif olasılıklar veya 1'den büyük olasılıklar elde edebilirim.<br /
<strong>Çözüm</strong>: <a href="extend-lm.html#glm">Genelleştirilmiş Lineer Modeller (GLM)</a>.</p>
            
<p><strong>Problem</strong>: Öznitelikler birbirleriyle etkileşim halinde.<br />
<strong>Örnek</strong>: Ortalamada hafif yağışın bisiklet sürme isteğime küçük negatif bir etkisi var. Ama yazın, yoğun saatlerde, yağmur yağması benim
  için iyi, çünkü böyle durumlarda açık havada bisiklet sürmek isteyen bisikletçiler evlerinden çıkmaz ve tüm bisiklet yolları bana kalır! Bu, zaman ve hava
  öznitelikleri arasında bir etkileşime örnektir ve sırf eklemeli bir modelle açıklanamaz.<br />
<strong>Çözüm</strong>: <a href="extend-lm.html#lm-interact">Etkileşim özniteliklerini elle eklemek</a>.</p>
            
<p><strong>Problem</strong>: Öznitelikler ve y arasındaki ilişki lineer değil.<br />
<strong>Örnek</strong>: 0 ve 25 derece Celsius arasında hava sıcaklığının benim bisiklet sürme isteğime etkisi lineer olabilir, yani bu aralıkta
  0 dereceden 1 dereceye artan hava sıcaklığı bisiklet sürme isteğimi 20'den 21'e çıkartır. Ama daha yüksek sıcaklıklarda bisiklet sürme
  isteğim düşmeye başlar, çok sıcak havalarda bisiklet sürmek istemem.<br />
<strong>Çözümler</strong>: <a href="extend-lm.html#gam">Genelleştirilmiş Arttırmalı Modeller (GAM); özniteliklerin dönüştürülmesi</a>.</p>
            
<p>Bu bölümde daha farklı lineer model genişlemelerini ihmal ederek bu üç çözümü inceleyeceğiz, çünkü her şeyi burada anlatmaya çalışırsam bu bölüm
  kitap içinde bir kitaba dönüşecektir, hem bu konu başka kitaplarda zaten anlatılmıştır. Ama, <a href="extend-lm.html#more-lm-extension">bölümün sonunda
  bulabileceğiniz kısımda</a>, diğer genişlemelere
  örnek olacak sorular ve cevapları listeledim. Çözümlerde gördüğünüz isimleri araştırarak daha fazla bilgi edinebilirsiniz.</p>
  
  The solutions to these three problems are presented in this chapter.
Many further extensions of the linear model are omitted.
If I attempted to cover everything here, the chapter would quickly turn into a book within a book about a topic that is already covered in many other books.
But since you are already here, I have made a little problem plus solution overview for linear model extensions, which you can find at
  the <a href="extend-lm.html#more-lm-extension">end of the chapter</a>.
The name of the solution is meant to serve as a starting point for a search.</p>
            
<div id="glm" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.3.1</span> Non-Gaussian Outcomes - GLMs<a href="extend-lm.html#glm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The linear regression model assumes that the outcome given the input features follows a Gaussian distribution.
This assumption excludes many cases:
The outcome can also be a category (cancer vs. healthy), a count (number of children), the time to the occurrence of an event (time to failure of a machine) or a very skewed outcome with a few very high values (household income).
The linear regression model can be extended to model all these types of outcomes.
This extension is called <strong>Generalized Linear Models</strong> or <strong>GLMs</strong> for short.
Throughout this chapter, I will use the name GLM for both the general framework and for particular models from that framework.
The core concept of any GLM is:
Keep the weighted sum of the features, but allow non-Gaussian outcome distributions and connect the expected mean of this distribution and the weighted sum through a possibly nonlinear function.
For example, the logistic regression model assumes a Bernoulli distribution for the outcome and links the expected mean and the weighted sum using the logistic function.</p>
<p>The GLM mathematically links the weighted sum of the features with the mean value of the assumed distribution using the link function g, which can be chosen flexibly depending on the type of outcome.</p>
<p><span class="math display">\[g(E_Y(y|x))=\beta_0+\beta_1{}x_{1}+\ldots{}\beta_p{}x_{p}\]</span></p>
<p>GLMs consist of three components:
The link function g, the weighted sum <span class="math inline">\(X^T\beta\)</span> (sometimes called linear predictor) and a probability distribution from the exponential family that defines <span class="math inline">\(E_Y\)</span>.</p>
<p>The exponential family is a set of distributions that can be written with the same (parameterized) formula that includes an exponent, the mean and variance of the distribution and some other parameters.
I will not go into the mathematical details because this is a very big universe of its own that I do not want to enter.
Wikipedia has a neat <a href="https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions">list of distributions from the exponential family</a>.
Any distribution from this list can be chosen for your GLM.
Based on the type of the outcome you want to predict, choose a suitable distribution.
Is the outcome a count of something (e.g. number of children living in a household)?
Then the Poisson distribution could be a good choice.
Is the outcome always positive (e.g. time between two events)?
Then the exponential distribution could be a good choice.</p>
<p>Let us consider the classic linear model as a special case of a GLM.
The link function for the Gaussian distribution in the classic linear model is simply the identity function.
The Gaussian distribution is parameterized by the mean and the variance parameters.
The mean describes the value that we expect on average and the variance describes how much the values vary around this mean.
In the linear model, the link function links the weighted sum of the features to the mean of the Gaussian distribution.</p>
<p>Under the GLM framework, this concept generalizes to any distribution (from the exponential family) and arbitrary link functions.
If y is a count of something, such as the number of coffees someone drinks on a certain day, we could model it with a GLM with a Poisson distribution and the natural logarithm as the link function:</p>
<p><span class="math display">\[ln(E_Y(y|x))=x^{T}\beta\]</span></p>
<p>The logistic regression model is also a GLM that assumes a Bernoulli distribution and uses the logit function as the link function.
The mean of the binomial distribution used in logistic regression is the probability that y is 1.</p>
<p><span class="math display">\[x^{T}\beta=ln\left(\frac{E_Y(y|x)}{1-E_Y(y|x)}\right)=ln\left(\frac{P(y=1|x)}{1-P(y=1|x)}\right)\]</span></p>
<p>And if we solve this equation to have P(y=1) on one side, we get the logistic regression formula:</p>
<p><span class="math display">\[P(y=1)=\frac{1}{1+exp(-x^{T}\beta)}\]</span></p>
<p>Each distribution from the exponential family has a canonical link function that can be derived mathematically from the distribution.
The GLM framework makes it possible to choose the link function independently of the distribution.
How to choose the right link function?
There is no perfect recipe.
You take into account knowledge about the distribution of your target, but also theoretical considerations and how well the model fits your actual data.
For some distributions the canonical link function can lead to values that are invalid for that distribution.
In the case of the exponential distribution, the canonical link function is the negative inverse, which can lead to negative predictions that are outside the domain of the exponential distribution.
Since you can choose any link function, the simple solution is to choose another function that respects the domain of the distribution.</p>
<p><strong>Examples</strong></p>
<p>I have simulated a dataset on coffee drinking behavior to highlight the need for GLMs.
Suppose you have collected data about your daily coffee drinking behavior.
If you do not like coffee, pretend it is about tea.
Along with number of cups, you record your current stress level on a scale of 1 to 10, how well you slept the night before on a scale of 1 to 10 and whether you had to work on that day.
The goal is to predict the number of coffees given the features stress, sleep and work.
I simulated data for 200 days.
Stress and sleep were drawn uniformly between 1 and 10 and work yes/no was drawn with a 50/50 chance (what a life!).
For each day, the number of coffees was then drawn from a Poisson distribution, modelling the intensity <span class="math inline">\(\lambda\)</span> (which is also the expected value of the Poisson distribution) as a function of the features sleep, stress and work.
You can guess where this story will lead:
<em>“Hey, let us model this data with a linear model … Oh it does not work … Let us try a GLM with Poisson distribution … SURPRISE! Now it works!”.</em>
I hope I did not spoil the story too much for you.</p>
<p>Let us look at the distribution of the target variable, the number of coffees on a given day:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poisson-data"></span>
<img src="images/poisson-data-1.png" alt="Simulated distribution of number of daily coffees for 200 days." width="\textwidth" />
<p class="caption">
FIGURE 5.9: Simulated distribution of number of daily coffees for 200 days.
</p>
</div>
<p>On 76 of the 200 days you had no coffee at all and on the most extreme day you had 7.
Let us naively use a linear model to predict the number of coffees using sleep level, stress level and work yes/no as features.
What can go wrong when we falsely assume a Gaussian distribution?
A wrong assumption can invalidate the estimates, especially the confidence intervals of the weights.
A more obvious problem is that the predictions do not match the “allowed” domain of the true outcome, as the following figure shows.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:failing-linear-model"></span>
<img src="images/failing-linear-model-1.png" alt="Predicted number of coffees dependent on stress, sleep and work. The linear model predicts negative values." width="\textwidth" />
<p class="caption">
FIGURE 5.10: Predicted number of coffees dependent on stress, sleep and work. The linear model predicts negative values.
</p>
</div>
<p>The linear model does not make sense, because it predicts negative number of coffees.
This problem can be solved with Generalized Linear Models (GLMs).
We can change the link function and the assumed distribution.
One possibility is to keep the Gaussian distribution and use a link function that always leads to positive predictions such as the log-link (the inverse is the exp-function) instead of the identity function.
Even better:
We choose a distribution that corresponds to the data generating process and an appropriate link function.
Since the outcome is a count, the Poisson distribution is a natural choice, along with the logarithm as link function.
In this case, the data was even generated with the Poisson distribution, so the Poisson GLM is the perfect choice.
The fitted Poisson GLM leads to the following distribution of predicted values:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linear-model-positive"></span>
<img src="images/linear-model-positive-1.png" alt="Predicted number of coffees dependent on stress, sleep and work. The GLM with Poisson assumption and log link is an appropriate model for this dataset." width="\textwidth" />
<p class="caption">
FIGURE 5.11: Predicted number of coffees dependent on stress, sleep and work. The GLM with Poisson assumption and log link is an appropriate model for this dataset.
</p>
</div>
<p>No negative amounts of coffees, looks much better now.</p>
<p><strong>Interpretation of GLM weights</strong></p>
<p>The assumed distribution together with the link function determines how the estimated feature weights are interpreted.
In the coffee count example, I used a GLM with Poisson distribution and log link, which implies the following relationship between the expected outcome and the features stress (str), sleep (slp) and work (wrk).</p>
<p><span class="math display">\[ln(E(\text{coffee}|\text{str},\text{slp},\text{wrk}))=\beta_0+\beta_{\text{str}}x_{\text{str}}+\beta_{\text{slp}}x_{\text{slp}}+\beta_{\text{wrk}}x_{\text{wrk}}\]</span></p>
<p>To interpret the weights we invert the link function so that we can interpret the effect of the features on the expected outcome and not on the logarithm of the expected outcome.</p>
<p><span class="math display">\[E(\text{coffee}|\text{str},\text{slp},\text{wrk})=exp(\beta_0+\beta_{\text{str}}x_{\text{str}}+\beta_{\text{slp}}x_{\text{slp}}+\beta_{\text{wrk}}x_{\text{wrk}})\]</span></p>
<p>Since all the weights are in the exponential function, the effect interpretation is not additive, but multiplicative, because exp(a + b) is exp(a) times exp(b).
The last ingredient for the interpretation is the actual weights of the toy example.
The following table lists the estimated weights and exp(weights) together with the 95% confidence interval:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:poisson-model-params">TABLE 5.3: </span>Weights in the Poisson model
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
weight
</th>
<th style="text-align:left;">
exp(weight) [2.5%, 97.5%]
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
-0.16
</td>
<td style="text-align:left;">
0.85 [0.54, 1.32]
</td>
</tr>
<tr>
<td style="text-align:left;">
stress
</td>
<td style="text-align:right;">
0.12
</td>
<td style="text-align:left;">
1.12 [1.07, 1.18]
</td>
</tr>
<tr>
<td style="text-align:left;">
sleep
</td>
<td style="text-align:right;">
-0.15
</td>
<td style="text-align:left;">
0.86 [0.82, 0.90]
</td>
</tr>
<tr>
<td style="text-align:left;">
workYES
</td>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:left;">
2.23 [1.72, 2.93]
</td>
</tr>
</tbody>
</table>
<p>Increasing the stress level by one point multiplies the expected number of coffees by the factor 1.12.
Increasing the sleep quality by one point multiplies the expected number of coffees by the factor 0.86.
The predicted number of coffees on a work day is on average 2.23 times the number of coffees on a day off.
In summary, the more stress, the less sleep and the more work, the more coffee is consumed.</p>
<p>In this section you learned a little about Generalized Linear Models that are useful when the target does not follow a Gaussian distribution.
Next, we look at how to integrate interactions between two features into the linear regression model.</p>
</div>
<div id="lm-interact" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.3.2</span> Interactions<a href="extend-lm.html#lm-interact" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The linear regression model assumes that the effect of one feature is the same regardless of the values of the other features (= no interactions).
But often there are interactions in the data.
To predict the <a href="bike-data.html#bike-data">number of bicycles</a> rented, there may be an interaction between temperature and whether it is a working day or not.
Perhaps, when people have to work, the temperature does not influence the number of rented bikes much, because people will ride the rented bike to work no matter what happens.
On days off, many people ride for pleasure, but only when it is warm enough.
When it comes to rental bicycles, you might expect an interaction between temperature and working day.</p>
<p>How can we get the linear model to include interactions?
Before you fit the linear model, add a column to the feature matrix that represents the interaction between the features and fit the model as usual.
The solution is elegant in a way, since it does not require any change of the linear model, only additional columns in the data.
In the working day and temperature example, we would add a new feature that has zeros for no-work days, otherwise it has the value of the temperature feature, assuming that working day is the reference category.
Suppose our data looks like this:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
work
</th>
<th style="text-align:right;">
temp
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Y
</td>
<td style="text-align:right;">
25
</td>
</tr>
<tr>
<td style="text-align:left;">
N
</td>
<td style="text-align:right;">
12
</td>
</tr>
<tr>
<td style="text-align:left;">
N
</td>
<td style="text-align:right;">
30
</td>
</tr>
<tr>
<td style="text-align:left;">
Y
</td>
<td style="text-align:right;">
5
</td>
</tr>
</tbody>
</table>
<p>The data matrix used by the linear model looks slightly different.
The following table shows what the data prepared for the model looks like if we do not specify any interactions.
Normally, this transformation is performed automatically by any statistical software.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
Intercept
</th>
<th style="text-align:right;">
workY
</th>
<th style="text-align:right;">
temp
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
25
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
12
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
30
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5
</td>
</tr>
</tbody>
</table>
<p>The first column is the intercept term.
The second column encodes the categorical feature, with 0 for the reference category and 1 for the other.
The third column contains the temperature.</p>
<p>If we want the linear model to consider the interaction between temperature and the workingday feature, we have to add a column for the interaction:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
Intercept
</th>
<th style="text-align:right;">
workY
</th>
<th style="text-align:right;">
temp
</th>
<th style="text-align:right;">
workY.temp
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
25
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
</tr>
</tbody>
</table>
<p>The new column “workY.temp” captures the interaction between the features working day (work) and temperature (temp).
This new feature column is zero for an instance if the work feature is at the reference category (“N” for no working day), otherwise it assumes the values of the instances temperature feature.
With this type of encoding, the linear model can learn a different linear effect of temperature for both types of days.
This is the interaction effect between the two features.
Without an interaction term, the combined effect of a categorical and a numerical feature can be described by a line that is vertically shifted for the different categories.
If we include the interaction, we allow the effect of the numerical features (the slope) to have a different value in each category.</p>
<p>The interaction of two categorical features works similarly.
We create additional features which represent combinations of categories.
Here is some artificial data containing working day (work) and a categorical weather feature (wthr):</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
work
</th>
<th style="text-align:left;">
wthr
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Y
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
N
</td>
<td style="text-align:left;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
N
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
Y
</td>
<td style="text-align:left;">
2
</td>
</tr>
</tbody>
</table>
<p>Next, we include interaction terms:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
Intercept
</th>
<th style="text-align:right;">
workY
</th>
<th style="text-align:right;">
wthr1
</th>
<th style="text-align:right;">
wthr2
</th>
<th style="text-align:right;">
workY.wthr1
</th>
<th style="text-align:right;">
workY.wthr2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
</table>
<p>The first column serves to estimate the intercept.
The second column is the encoded work feature.
Columns three and four are for the weather feature, which requires two columns because you need two weights to capture the effect for three categories, one of which is the reference category.
The rest of the columns capture the interactions.
For each category of both features (except for the reference categories), we create a new feature column that is 1 if both features have a certain category, otherwise 0.</p>
<p>For two numerical features, the interaction column is even easier to construct:
We simply multiply both numerical features.</p>
<p>There are approaches to automatically detect and add interaction terms.
One of them can be found in the <a href="rulefit.html#rulefit">RuleFit chapter</a>.
The RuleFit algorithm first mines interaction terms and then estimates a linear regression model including interactions.</p>
<p><strong>Example</strong></p>
<p>Let us return to the <a href="bike-data.html#bike-data">bike rental prediction task</a> which we have already modeled in the <a href="limo.html#limo">linear model chapter</a>.
This time, we additionally consider an interaction between the temperature and the working day feature.
This results in the following estimated weights and confidence intervals.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Weight
</th>
<th style="text-align:right;">
Std. Error
</th>
<th style="text-align:right;">
2.5%
</th>
<th style="text-align:right;">
97.5%
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
2185.8
</td>
<td style="text-align:right;">
250.2
</td>
<td style="text-align:right;">
1694.6
</td>
<td style="text-align:right;">
2677.1
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonSPRING
</td>
<td style="text-align:right;">
893.8
</td>
<td style="text-align:right;">
121.8
</td>
<td style="text-align:right;">
654.7
</td>
<td style="text-align:right;">
1132.9
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonSUMMER
</td>
<td style="text-align:right;">
137.1
</td>
<td style="text-align:right;">
161.0
</td>
<td style="text-align:right;">
-179.0
</td>
<td style="text-align:right;">
453.2
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonFALL
</td>
<td style="text-align:right;">
426.5
</td>
<td style="text-align:right;">
110.3
</td>
<td style="text-align:right;">
209.9
</td>
<td style="text-align:right;">
643.2
</td>
</tr>
<tr>
<td style="text-align:left;">
holidayHOLIDAY
</td>
<td style="text-align:right;">
-674.4
</td>
<td style="text-align:right;">
202.5
</td>
<td style="text-align:right;">
-1071.9
</td>
<td style="text-align:right;">
-276.9
</td>
</tr>
<tr>
<td style="text-align:left;">
workingdayWORKING DAY
</td>
<td style="text-align:right;">
451.9
</td>
<td style="text-align:right;">
141.7
</td>
<td style="text-align:right;">
173.7
</td>
<td style="text-align:right;">
730.1
</td>
</tr>
<tr>
<td style="text-align:left;">
weathersitMISTY
</td>
<td style="text-align:right;">
-382.1
</td>
<td style="text-align:right;">
87.2
</td>
<td style="text-align:right;">
-553.3
</td>
<td style="text-align:right;">
-211.0
</td>
</tr>
<tr>
<td style="text-align:left;">
weathersitRAIN/…
</td>
<td style="text-align:right;">
-1898.2
</td>
<td style="text-align:right;">
222.7
</td>
<td style="text-align:right;">
-2335.4
</td>
<td style="text-align:right;">
-1461.0
</td>
</tr>
<tr>
<td style="text-align:left;">
temp
</td>
<td style="text-align:right;">
125.4
</td>
<td style="text-align:right;">
8.9
</td>
<td style="text-align:right;">
108.0
</td>
<td style="text-align:right;">
142.9
</td>
</tr>
<tr>
<td style="text-align:left;">
hum
</td>
<td style="text-align:right;">
-17.5
</td>
<td style="text-align:right;">
3.2
</td>
<td style="text-align:right;">
-23.7
</td>
<td style="text-align:right;">
-11.3
</td>
</tr>
<tr>
<td style="text-align:left;">
windspeed
</td>
<td style="text-align:right;">
-42.1
</td>
<td style="text-align:right;">
6.9
</td>
<td style="text-align:right;">
-55.5
</td>
<td style="text-align:right;">
-28.6
</td>
</tr>
<tr>
<td style="text-align:left;">
days_since_2011
</td>
<td style="text-align:right;">
4.9
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
4.6
</td>
<td style="text-align:right;">
5.3
</td>
</tr>
<tr>
<td style="text-align:left;">
workingdayWORKING DAY:temp
</td>
<td style="text-align:right;">
-21.8
</td>
<td style="text-align:right;">
8.1
</td>
<td style="text-align:right;">
-37.7
</td>
<td style="text-align:right;">
-5.9
</td>
</tr>
</tbody>
</table>
<p>The additional interaction effect is negative (-21.8) and differs significantly from zero, as shown by the 95% confidence interval, which does not include zero.
By the way, the data are not iid, because days that are close to each other are not independent from each other.
Confidence intervals might be misleading, just take it with a grain of salt.
The interaction term changes the interpretation of the weights of the involved features.
Does the temperature have a negative effect given it is a working day?
The answer is no, even if the table suggests it to an untrained user.
We cannot interpret the “workingdayWORKING DAY:temp” interaction weight in isolation, since the interpretation would be:
“While leaving all other feature values unchanged, increasing the interaction effect of temperature for working day decreases the predicted number of bikes.”
But the interaction effect only adds to the main effect of the temperature.
Suppose it is a working day and we want to know what would happen if the temperature were 1 degree warmer today.
Then we need to sum both the weights for “temp” and “workingdayWORKING DAY:temp” to determine how much the estimate increases.</p>
<p>It is easier to understand the interaction visually.
By introducing an interaction term between a categorical and a numerical feature, we get two slopes for the temperature instead of one.
The temperature slope for days on which people do not have to work (‘NO WORKING DAY’) can be read directly from the table (125.4).
The temperature slope for days on which people have to work (‘WORKING DAY’) is the sum of both temperature weights (125.4 -21.8 = 103.6).
The intercept of the ‘NO WORKING DAY’-line at temperature = 0 is determined by the intercept term of the linear model (2185.8).
The intercept of the ‘WORKING DAY’-line at temperature = 0 is determined by the intercept term + the effect of working day (2185.8 + 451.9 = 2637.7).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:interaction-plot"></span>
<img src="images/interaction-plot-1.png" alt="The effect (including interaction) of temperature and working day on the predicted number of bikes for a linear model. Effectively, we get two slopes for the temperature, one for each category of the working day feature." width="\textwidth" />
<p class="caption">
FIGURE 5.12: The effect (including interaction) of temperature and working day on the predicted number of bikes for a linear model. Effectively, we get two slopes for the temperature, one for each category of the working day feature.
</p>
</div>
</div>
<div id="gam" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.3.3</span> Nonlinear Effects - GAMs<a href="extend-lm.html#gam" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>The world is not linear.</strong>
Linearity in linear models means that no matter what value an instance has in a particular feature, increasing the value by one unit always has the same effect on the predicted outcome.
Is it reasonable to assume that increasing the temperature by one degree at 10 degrees Celsius has the same effect on the number of rental bikes as increasing the temperature when it already has 40 degrees?
Intuitively, one expects that increasing the temperature from 10 to 11 degrees Celsius has a positive effect on bicycle rentals and from 40 to 41 a negative effect, which is also the case, as you will see, in many examples throughout the book.
The temperature feature has a linear, positive effect on the number of rental bikes, but at some point it flattens out and even has a negative effect at high temperatures.
The linear model does not care, it will dutifully find the best linear plane (by minimizing the Euclidean distance).</p>
<p>You can model nonlinear relationships using one of the following techniques:</p>
<ul>
<li>Simple transformation of the feature (e.g. logarithm)</li>
<li>Categorization of the feature</li>
<li>Generalized Additive Models (GAMs)</li>
</ul>
<p>Before I go into the details of each method, let us start with an example that illustrates all three of them.
I took the <a href="bike-data.html#bike-data">bike rental dataset</a> and trained a linear model with only the temperature feature to predict the number of rental bikes.
The following figure shows the estimated slope with: the standard linear model, a linear model with transformed temperature (logarithm), a linear model with temperature treated as categorical feature and using regression splines (GAM).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nonlinear-effects"></span>
<img src="images/nonlinear-effects-1.png" alt="Predicting the number of rented bicycles using only the temperature feature. A linear model (top left) does not fit the data well. One solution is to transform the feature with e.g. the logarithm (top right), categorize it (bottom left), which is usually a bad decision, or use Generalized Additive Models that can automatically fit a smooth curve for temperature (bottom right)." width="\textwidth" />
<p class="caption">
FIGURE 5.13: Predicting the number of rented bicycles using only the temperature feature. A linear model (top left) does not fit the data well. One solution is to transform the feature with e.g. the logarithm (top right), categorize it (bottom left), which is usually a bad decision, or use Generalized Additive Models that can automatically fit a smooth curve for temperature (bottom right).
</p>
</div>
<p><strong>Feature transformation</strong></p>
<p>Often the logarithm of the feature is used as a transformation.
Using the logarithm indicates that every 10-fold temperature increase has the same linear effect on the number of bikes, so changing from 1 degree Celsius to 10 degrees Celsius has the same effect as changing from 0.1 to 1 (sounds wrong).
Other examples for feature transformations are the square root, the square function and the exponential function.
Using a feature transformation means that you replace the column of this feature in the data with a function of the feature, such as the logarithm, and fit the linear model as usual.
Some statistical programs also allow you to specify transformations in the call of the linear model.
You can be creative when you transform the feature.
The interpretation of the feature changes according to the selected transformation.
If you use a log transformation, the interpretation in a linear model becomes:
“If the logarithm of the feature is increased by one, the prediction is increased by the corresponding weight.”
When you use a GLM with a link function that is not the identity function, then the interpretation gets more complicated, because you have to incorporate both transformations into the interpretation (except when they cancel each other out, like log and exp, then the interpretation gets easier).</p>
<p><strong>Feature categorization</strong></p>
<p>Another possibility to achieve a nonlinear effect is to discretize the feature; turn it into a categorical feature.
For example, you could cut the temperature feature into 20 intervals with the levels [-10, -5), [-5, 0), … and so on.
When you use the categorized temperature instead of the continuous temperature, the linear model would estimate a step function because each level gets its own estimate.
The problem with this approach is that it needs more data, it is more likely to overfit and it is unclear how to discretize the feature meaningfully (equidistant intervals or quantiles? how many intervals?).
I would only use discretization if there is a very strong case for it.
For example, to make the model comparable to another study.</p>
<p><strong>Generalized Additive Models (GAMs)</strong></p>
<p>Why not ‘simply’ allow the (generalized) linear model to learn nonlinear relationships?
That is the motivation behind GAMs.
GAMs relax the restriction that the relationship must be a simple weighted sum, and instead assume that the outcome can be modeled by a sum of arbitrary functions of each feature.
Mathematically, the relationship in a GAM looks like this:</p>
<p><span class="math display">\[g(E_Y(y|x))=\beta_0+f_1(x_{1})+f_2(x_{2})+\ldots+f_p(x_{p})\]</span></p>
<p>The formula is similar to the GLM formula with the difference that the linear term <span class="math inline">\(\beta_j{}x_{j}\)</span> is replaced by a more flexible function <span class="math inline">\(f_j(x_{j})\)</span>.
The core of a GAM is still a sum of feature effects, but you have the option to allow nonlinear relationships between some features and the output.
Linear effects are also covered by the framework, because for features to be handled linearly, you can limit their <span class="math inline">\(f_j(x_{j})\)</span> only to take the form of <span class="math inline">\(x_{j}\beta_j\)</span>.</p>
<p>The big question is how to learn nonlinear functions.
The answer is called “splines” or “spline functions”.
Splines are functions that are constructed from simpler basis functions.
Splines can be used to approximate other, more complex functions.
A bit like stacking Lego bricks to build something more complex.
There is a confusing number of ways to define these spline basis functions.
If you are interested in learning more about all the ways to define basis functions, I wish you good luck on your journey.
I am not going to go into details here, I am just going to build an intuition.
What personally helped me the most for understanding splines was to visualize the individual basis functions and to look into how the data matrix is modified.
For example, to model the temperature with splines, we remove the temperature feature from the data and replace it with, say, 4 columns, each representing a spline basis function.
Usually you would have more spline basis functions, I only reduced the number for illustration purposes.
The value for each instance of these new spline basis features depends on the instances’ temperature values.
Together with all linear effects, the GAM then also estimates these spline weights.
GAMs also introduce a penalty term for the weights to keep them close to zero.
This effectively reduces the flexibility of the splines and reduces overfitting.
A smoothness parameter that is commonly used to control the flexibility of the curve is then tuned via cross-validation.
Ignoring the penalty term, nonlinear modeling with splines is fancy feature engineering.</p>
<p>In the example where we are predicting the number of bicycles with a GAM using only the temperature, the model feature matrix looks like this:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
(Intercept)
</th>
<th style="text-align:right;">
s(temp).1
</th>
<th style="text-align:right;">
s(temp).2
</th>
<th style="text-align:right;">
s(temp).3
</th>
<th style="text-align:right;">
s(temp).4
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.93
</td>
<td style="text-align:right;">
-0.14
</td>
<td style="text-align:right;">
0.21
</td>
<td style="text-align:right;">
-0.83
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.83
</td>
<td style="text-align:right;">
-0.27
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:right;">
-0.72
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.32
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
-0.39
</td>
<td style="text-align:right;">
-1.63
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.32
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
-0.38
</td>
<td style="text-align:right;">
-1.61
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.29
</td>
<td style="text-align:right;">
0.58
</td>
<td style="text-align:right;">
-0.26
</td>
<td style="text-align:right;">
-1.47
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.32
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
-0.36
</td>
<td style="text-align:right;">
-1.59
</td>
</tr>
</tbody>
</table>
<p>Each row represents an individual instance from the data (one day).
Each spline basis column contains the value of the spline basis function at the particular temperature values.
The following figure shows how these spline basis functions look like:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:splines"></span>
<img src="images/splines-1.png" alt="To smoothly model the temperature effect, we use 4 spline basis functions. Each temperature value is mapped to (here) 4 spline basis values. If an instance has a temperature of 30 °C, the value for the first spline basis feature is -1, for the second 0.7, for the third -0.8 and for the 4th 1.7." width="\textwidth" />
<p class="caption">
FIGURE 5.14: To smoothly model the temperature effect, we use 4 spline basis functions. Each temperature value is mapped to (here) 4 spline basis values. If an instance has a temperature of 30 °C, the value for the first spline basis feature is -1, for the second 0.7, for the third -0.8 and for the 4th 1.7.
</p>
</div>
<p>The GAM assigns weights to each temperature spline basis feature:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
weight
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
4504.35
</td>
</tr>
<tr>
<td style="text-align:left;">
s(temp).1
</td>
<td style="text-align:right;">
-989.34
</td>
</tr>
<tr>
<td style="text-align:left;">
s(temp).2
</td>
<td style="text-align:right;">
740.08
</td>
</tr>
<tr>
<td style="text-align:left;">
s(temp).3
</td>
<td style="text-align:right;">
2309.84
</td>
</tr>
<tr>
<td style="text-align:left;">
s(temp).4
</td>
<td style="text-align:right;">
558.27
</td>
</tr>
</tbody>
</table>
<p>And the actual spline curve, which results from the sum of the spline basis functions weighted with the estimated weights, looks like this:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:splines-curve"></span>
<img src="images/splines-curve-1.png" alt="GAM feature effect of the temperature for predicting the number of rented bikes (temperature used as the only feature)." width="\textwidth" />
<p class="caption">
FIGURE 5.15: GAM feature effect of the temperature for predicting the number of rented bikes (temperature used as the only feature).
</p>
</div>
<p>The interpretation of smooth effects requires a visual check of the fitted curve.
Splines are usually centered around the mean prediction, so a point on the curve is the difference to the mean prediction.
For example, at 0 degrees Celsius, the predicted number of bicycles is 3000 lower than the average prediction.</p>
</div>
<div id="advantages-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.3.4</span> Advantages<a href="extend-lm.html#advantages-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>All these extensions of the linear model are a bit of a universe in themselves.
Whatever problems you face with linear models, <strong>you will probably find an extension that fixes it</strong>.</p>
<p>Most methods have been used for decades.
For example, GAMs are almost 30 years old.
Many researchers and practitioners from industry are very <strong>experienced</strong> with linear models and the methods are <strong>accepted in many communities as status quo for modeling</strong>.</p>
<p>In addition to making predictions, you can use the models to <strong>do inference</strong>, draw conclusions about the data – given the model assumptions are not violated.
You get confidence intervals for weights, significance tests, prediction intervals and much more.</p>
<p>Statistical software usually has really good interfaces to fit GLMs, GAMs and more special linear models.</p>
<p>The opacity of many machine learning models comes from 1) a lack of sparseness, which means that many features are used, 2) features that are treated in a nonlinear fashion, which means you need more than a single weight to describe the effect, and 3) the modeling of interactions between the features.
Assuming that linear models are highly interpretable but often underfit reality, the extensions described in this chapter offer a good way to achieve a <strong>smooth transition to more flexible models</strong>, while preserving some of the interpretability.</p>
</div>
<div id="disadvantages-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.3.5</span> Disadvantages<a href="extend-lm.html#disadvantages-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As advantage I have said that linear models live in their own universe.
The sheer <strong>number of ways you can extend the simple linear model is overwhelming</strong>, not just for beginners.
Actually, there are multiple parallel universes, because many communities of researchers and practitioners have their own names for methods that do more or less the same thing, which can be very confusing.</p>
<p>Most modifications of the linear model make the model <strong>less interpretable</strong>.
Any link function (in a GLM) that is not the identity function complicates the interpretation;
interactions also complicate the interpretation;
nonlinear feature effects are either less intuitive (like the log transformation) or can no longer be summarized by a single number (e.g. spline functions).</p>
<p>GLMs, GAMs and so on <strong>rely on assumptions</strong> about the data generating process.
If those are violated, the interpretation of the weights is no longer valid.</p>
<p>The performance of tree-based ensembles like the random forest or gradient tree boosting is in many cases better than the most sophisticated linear models.
This is partly my own experience and partly observations from the winning models on platforms like kaggle.com.</p>
</div>
<div id="software-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.3.6</span> Software<a href="extend-lm.html#software-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>All examples in this chapter were created using the R language.
For GAMs, the <code>gam</code> package was used, but there are many others.
R has an incredible number of packages to extend linear regression models.
Unsurpassed by any other analytics language, R is home to every conceivable extension of the linear regression model extension.
You will find implementations of e.g. GAMs in Python (such as <a href="https://github.com/dswah/pyGAM">pyGAM</a>), but these implementations are not as mature.</p>
</div>
<div id="more-lm-extension" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.3.7</span> Further Extensions<a href="extend-lm.html#more-lm-extension" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As promised, here is a list of problems you might encounter with linear models, along with the name of a solution for this problem that you can copy and paste into your favorite search engine.</p>
<p>My data violates the assumption of being independent and identically distributed (iid).<br />
For example, repeated measurements on the same patient.<br />
Search for <strong>mixed models</strong> or <strong>generalized estimating equations</strong>.</p>
<p>My model has heteroscedastic errors.<br />
For example, when predicting the value of a house, the model errors are usually higher in expensive houses, which violates the homoscedasticity of the linear model.<br />
Search for <strong>robust regression</strong>.</p>
<p>I have outliers that strongly influence my model.<br />
Search for <strong>robust regression</strong>.</p>
<p>I want to predict the time until an event occurs.<br />
Time-to-event data usually comes with censored measurements, which means that for some instances there was not enough time to observe the event.
For example, a company wants to predict the failure of its ice machines, but only has data for two years.
Some machines are still intact after two years, but might fail later.<br />
Search for <strong>parametric survival models</strong>, <strong>cox regression</strong>, <strong>survival analysis</strong>.</p>
<p>My outcome to predict is a category.<br />
If the outcome has two categories use a <a href="logistic.html#logistic">logistic regression model</a>, which models the probability for the categories.<br />
If you have more categories, search for <strong>multinomial regression</strong>.<br />
Logistic regression and multinomial regression are both GLMs.</p>
<p>I want to predict ordered categories.<br />
For example school grades.<br />
Search for <strong>proportional odds model</strong>.</p>
<p>My outcome is a count (like number of children in a family).<br />
Search for <strong>Poisson regression</strong>.<br />
The Poisson model is also a GLM.
You might also have the problem that the count value of 0 is very frequent.<br />
Search for <strong>zero-inflated Poisson regression</strong>, <strong>hurdle model</strong>.</p>
<p>I am not sure what features need to be included in the model to draw correct causal conclusions.<br />
For example, I want to know the effect of a drug on the blood pressure.
The drug has a direct effect on some blood value and this blood value affects the outcome.
Should I include the blood value into the regression model?<br />
Search for <strong>causal inference</strong>, <strong>mediation analysis</strong>.</p>
<p>I have missing data.<br />
Search for <strong>multiple imputation</strong>.</p>
<p>I want to integrate prior knowledge into my models.<br />
Search for <strong>Bayesian inference</strong>.</p>
<p>I am feeling a bit down lately.<br />
Search for <strong>“Amazon Alexa Gone Wild!!! Full version from beginning to end”</strong>.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tree.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/manuscript/04.4-interpretable-lm-extensions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["interpretable-ml.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
