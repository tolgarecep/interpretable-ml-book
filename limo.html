<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.1 Lineer Regression | Yorumlanabilir Makine Öğrenmesi</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="5.1 Linear Regression | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.1 Linear Regression | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2022-03-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple.html"/>
<link rel="next" href="logistic.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>

<style>

#cta-button-desktop:hover, #cta-button-device:hover {
  background-color:   #ffc266; 
  border-color:   #ffc266; 
  box-shadow: none;
}
#cta-button-desktop, #cta-button-device{
  color: white;
  background-color:  #ffa31a;
  text-shadow:1px 1px 0 #444;
  text-decoration: none;
  border: 2px solid  #ffa31a;
  border-radius: 10px;
  position: fixed;
  padding: 5px 10px;
  z-index: 10;
  }

#cta-button-device {
  box-shadow: 0px 10px 10px -5px rgba(194,180,190,1);
  display:none;
  right: 20px;
  bottom: 20px;
  font-size: 20px;
 }

#cta-button-desktop {
  box-shadow: 0px 20px 20px -10px rgba(194,180,190,1);
  display:display;
  padding: 8px 16px;
  right: 40px;
  bottom: 40px;
  font-size: 25px;
}

@media (max-width : 450px) {
  #cta-button-device {display:block;}
  #cta-button-desktop {display:none;}
}


</style>


<a id="cta-button-desktop" href="https://leanpub.com/interpretable-machine-learning" rel="noopener noreferrer" target="blank"> Buy Book </a>

<a id="cta-button-device" href="https://leanpub.com/interpretable-machine-learning" rel="noopener noreferrer" target="blank">Buy</a>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">
  <!--Interpretable machine learning-->
  Yorumlanabilir Makine Öğrenmesi
  </a></li>

<li class="divider"></li>
<li><a href="index.html#summary"><!--Summary-->Özet<span></span></a></li>
<li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="preface-by-the-author.html"><i class="fa fa-check"></i><b>1</b>
  <!--Preface by the Author-->Yazarın Önsözü<span></span></a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b><!--Introduction--> Giriş<span></span></a><ul>
<li class="chapter" data-level="2.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>2.1</b>
  <!--Story Time-->Hikaye Vakti<span></span></a><ul>
<li><a href="storytime.html#lightning-never-strikes-twice"><!--Lightning Never Strikes Twice-->Yıldırım Aynı Yere İki Kez Düşmez<span></span></a></li>
<li><a href="storytime.html#trust-fall">Trust Fall<span></span></a></li>
<li><a href="storytime.html#fermis-paperclips">Fermi’s Paperclips<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2.2</b>
  <!--What Is Machine Learning?--> Makine Öğrenmesi Nedir?<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>2.3</b>
  <!--Terminology-->Terminoloji<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b>
  <!--Interpretability-->Yorumlanabilirlik<span></span></a><ul>
<li class="chapter" data-level="3.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>3.1</b>
  <!--Importance of Interpretability--> Yorumlanabilirliğin Önemi<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="taxonomy-of-interpretability-methods.html"><a href="taxonomy-of-interpretability-methods.html"><i class="fa fa-check"></i><b>3.2</b>
  <!--Taxonomy of Interpretability Methods--> Yorumlama Metotlarının Sınıflandırılması<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>3.3</b>
  <!--Scope of Interpretability--> Yorumlanabilirliğin Kapsamı<span></span></a><ul>
<li class="chapter" data-level="3.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>3.3.1</b>
  <!--Algorithm Transparency--> Algoritmaların Şeffaflığı<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>3.3.2</b>
  Global, Holistic Model Interpretability<span></span></a></li>
<li class="chapter" data-level="3.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>3.3.3</b>
  Global Model Interpretability on a Modular Level<span></span></a></li>
<li class="chapter" data-level="3.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>3.3.4</b>
  Local Interpretability for a Single Prediction<span></span></a></li>
<li class="chapter" data-level="3.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-predictions"><i class="fa fa-check"></i><b>3.3.5</b>
  Local Interpretability for a Group of Predictions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="evaluation-of-interpretability.html"><a href="evaluation-of-interpretability.html"><i class="fa fa-check"></i><b>3.4</b>
  <!--Evaluation of Interpretability--> Yorumlanabilirliğin Değerlendirilmesi<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>3.5</b>
  <!--Properties of Explanations--> Açıklamaların Özellikleri<span></span></a></li>
<li class="chapter" data-level="3.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>3.6</b>
  <!--Human-friendly Explanations--> İnsan Dostu Açıklamalar<span></span></a><ul>
<li class="chapter" data-level="3.6.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>3.6.1</b>
  <!--What Is an Explanation?--> Açıklama Nedir?<span></span></a></li>
<li class="chapter" data-level="3.6.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>3.6.2</b>
  <!--What Is a Good Explanation?-->İyi Açıklama Nasıl Olur?<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b><!--Datasets--> Veriler<span></span></a><ul>
<li class="chapter" data-level="4.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>4.1</b>
  <!--Bike Rentals (Regression)--> Bisiklet Kiralama (Regression)<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>4.2</b>
  <!--YouTube Spam Comments (Text Classification)--> Youtube Spam Yorumlar (Metin Sınıflandırması)<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>4.3</b>
  <!--Risk Factors for Cervical Cancer (Classification)--> Rahim Ağzı Kanseri (Sınıflandırma)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> 
  <!--Interpretable Models-->Yorumlanabilir Modeller<span></span></a><ul>
<li class="chapter" data-level="5.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>5.1</b> 
  Lineer Regression<span></span></a><ul>
<li class="chapter" data-level="5.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>5.1.1</b> 
  <!--Interpretation--> Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>5.1.2</b>
  <!--Example--> Örnek<span></span></a></li>
<li class="chapter" data-level="5.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>5.1.3</b>
  <!--Visual Interpretation--> Görsel Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.1.4" data-path="limo.html"><a href="limo.html#explain-individual-predictions"><i class="fa fa-check"></i><b>5.1.4</b>
  <!--Explain Individual Predictions-->Özel Tahminleri Açıklama<span></span></a></li>
<li class="chapter" data-level="5.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>5.1.5</b>
  <!--Encoding of Categorical Features--> Kategorik Özniteliklerin Şifrelenmesi<span></span></a></li>
<li class="chapter" data-level="5.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>5.1.6</b>
  <!--Do Linear Models Create Good Explanations?--> Lineer Modeller İyi Açıklamalar Meydana Getirir Mi?<span></span></a></li>
<li class="chapter" data-level="5.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>5.1.7</b> 
  <!--Sparse Linear Models--> Aralıklı Lineer Modeller<span></span></a></li>
<li class="chapter" data-level="5.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>5.1.8</b> 
  <!--Advantages--> Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>5.1.9</b>
  <!--Disadvantages--> Dezavantajlar<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression<span></span></a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic.html"><a href="logistic.html#what-is-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>5.2.1</b>
  <!--What is Wrong with Linear Regression for Classification?--> Linear Regression ile Sınıflandırmanın Problemi Ne?<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic.html"><a href="logistic.html#theory"><i class="fa fa-check"></i><b>5.2.2</b>
  <!--Theory--> Teori<span></span></a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>5.2.3</b>
  <!--Interpretation--> Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.2.4" data-path="logistic.html"><a href="logistic.html#example-1"><i class="fa fa-check"></i><b>5.2.4</b>
  <!--Example--> Örnek<span></span></a></li>
<li class="chapter" data-level="5.2.5" data-path="logistic.html"><a href="logistic.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>5.2.5</b>
  <!--Advantages and Disadvantages--> Avantajlar ve Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.2.6" data-path="logistic.html"><a href="logistic.html#software"><i class="fa fa-check"></i><b>5.2.6</b>
  <!--Software--> Yazılım<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>5.3</b> 
  <!--GLM, GAM and more-->GLM, GAM ve daha fazlası<span></span></a><ul>
<li class="chapter" data-level="5.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>5.3.1</b>
  <!--Non-Gaussian Outcomes - GLMs--> Non-Gaussian Sonuçlar - GLM'ler<span></span></a></li>
<li class="chapter" data-level="5.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>5.3.2</b>
  <!--Interactions--> Etkileşimler<span></span></a></li>
<li class="chapter" data-level="5.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>5.3.3</b>
  <!--Nonlinear Effects - GAMs--> Non-lineer etkiler - GAM'lar<span></span></a></li>
<li class="chapter" data-level="5.3.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>5.3.4</b>
  <!--Advantages--> Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.3.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>5.3.5</b>
  <!--Disadvantages--> Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.3.6" data-path="extend-lm.html"><a href="extend-lm.html#software-1"><i class="fa fa-check"></i><b>5.3.6</b>
  <!--Software--> Yazılım<span></span></a></li>
<li class="chapter" data-level="5.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>5.3.7</b>
  <!--Further Extensions--> İlaveler<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>5.4</b>
  <!--Decision Tree--> Karar Ağaçları<span></span></a><ul>
<li class="chapter" data-level="5.4.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>5.4.1</b>
  <!--Interpretation--> Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.4.2" data-path="tree.html"><a href="tree.html#example-2"><i class="fa fa-check"></i><b>5.4.2</b>
  <!--Example--> Yorumlama<span></span></a></li>
<li class="chapter" data-level="5.4.3" data-path="tree.html"><a href="tree.html#advantages-2"><i class="fa fa-check"></i><b>5.4.3</b>
  <!--Advantages--> Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.4.4" data-path="tree.html"><a href="tree.html#disadvantages-2"><i class="fa fa-check"></i><b>5.4.4</b>
  <!--Disadvantages--> Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.4.5" data-path="tree.html"><a href="tree.html#software-2"><i class="fa fa-check"></i><b>5.4.5</b>
  <!--Software--> Yazılım<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>5.5</b>
  <!--Decision Rules--> Karar Kuralları<span></span></a><ul>
<li class="chapter" data-level="5.5.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>5.5.1</b>
  <!--Learn Rules from a Single Feature (OneR)--> Yalnızca Bir Öznitelikten Kural Öğrenme (OneR)<span></span></a></li>
<li class="chapter" data-level="5.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>5.5.2</b> Sequential Covering<span></span></a></li>
<li class="chapter" data-level="5.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>5.5.3</b> 
  <!--Bayesian Rule Lists-->Bayes-yan Kural Listeleri<span></span></a></li>
<li class="chapter" data-level="5.5.4" data-path="rules.html"><a href="rules.html#advantages-3"><i class="fa fa-check"></i><b>5.5.4</b>
  <!--Advantages--> Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.5.5" data-path="rules.html"><a href="rules.html#disadvantages-3"><i class="fa fa-check"></i><b>5.5.5</b>
  <!--Disadvantages--> Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.5.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>5.5.6</b>
  <!--Software and Alternatives--> Yazılım ve alternatifler<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>5.6</b> RuleFit<span></span></a><ul>
<li class="chapter" data-level="5.6.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>5.6.1</b>
  <!--Interpretation and Example--> Yorumlama ve Örnek<span></span></a></li>
<li class="chapter" data-level="5.6.2" data-path="rulefit.html"><a href="rulefit.html#theory-1"><i class="fa fa-check"></i><b>5.6.2</b>
  <!--Theory--> Teori<span></span></a></li>
<li class="chapter" data-level="5.6.3" data-path="rulefit.html"><a href="rulefit.html#advantages-4"><i class="fa fa-check"></i><b>5.6.3</b> 
  <!--Advantages-->Avantajlar<span></span></a></li>
<li class="chapter" data-level="5.6.4" data-path="rulefit.html"><a href="rulefit.html#disadvantages-4"><i class="fa fa-check"></i><b>5.6.4</b>
  <!--Disadvantages-->Dezavantajlar<span></span></a></li>
<li class="chapter" data-level="5.6.5" data-path="rulefit.html"><a href="rulefit.html#software-and-alternative"><i class="fa fa-check"></i><b>5.6.5</b>
  <!--Software and Alternative-->Yazıım ve alternatif<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>5.7</b>
  <!--Other Interpretable Models--> Diğer Yorumlanabilir Modeller<span></span></a><ul>
<li class="chapter" data-level="5.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>5.7.1</b>
  <!--Naive Bayes Classifier--> Naive Bayes ile Sınıflandırma<span></span></a></li>
<li class="chapter" data-level="5.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.7.2</b> K-Nearest Neighbors<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>6</b> 
  <!--Model-Agnostic Methods-->Modelden Bağımsız Metotlar<span></span></a></li>
<li class="chapter" data-level="7" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>7</b> 
  <!--Example-Based Explanations-->Örneklere Dayalı Açıklamalar<span></span></a></li>
<li class="chapter" data-level="8" data-path="global-methods.html"><a href="global-methods.html"><i class="fa fa-check"></i><b>8</b> 
  <!--Global Model-Agnostic Methods-->Modelden Bağımsız Evrensel Metotlar<span></span></a><ul>
<li class="chapter" data-level="8.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>8.1</b> 
  <!--Partial Dependence Plot (PDP)-->Kısmi Bağımlılık Grafiği (PDP)<span></span></a><ul>
<li class="chapter" data-level="8.1.1" data-path="pdp.html"><a href="pdp.html#pdp-based-feature-importance"><i class="fa fa-check"></i><b>8.1.1</b> 
  <!--PDP-based Feature Importance-->PDP Tabanlı Nitelik Değerleri<span></span></a></li>
<li class="chapter" data-level="8.1.2" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>8.1.2</b> Examples<span></span></a></li>
<li class="chapter" data-level="8.1.3" data-path="pdp.html"><a href="pdp.html#advantages-5"><i class="fa fa-check"></i><b>8.1.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.1.4" data-path="pdp.html"><a href="pdp.html#disadvantages-5"><i class="fa fa-check"></i><b>8.1.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.1.5" data-path="pdp.html"><a href="pdp.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>8.1.5</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>8.2</b> Accumulated Local Effects (ALE) Plot<span></span></a><ul>
<li class="chapter" data-level="8.2.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>8.2.1</b> Motivation and Intuition<span></span></a></li>
<li class="chapter" data-level="8.2.2" data-path="ale.html"><a href="ale.html#theory-2"><i class="fa fa-check"></i><b>8.2.2</b> Theory<span></span></a></li>
<li class="chapter" data-level="8.2.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>8.2.3</b> Estimation<span></span></a></li>
<li class="chapter" data-level="8.2.4" data-path="ale.html"><a href="ale.html#examples-1"><i class="fa fa-check"></i><b>8.2.4</b> Examples<span></span></a></li>
<li class="chapter" data-level="8.2.5" data-path="ale.html"><a href="ale.html#advantages-6"><i class="fa fa-check"></i><b>8.2.5</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.2.6" data-path="ale.html"><a href="ale.html#disadvantages-6"><i class="fa fa-check"></i><b>8.2.6</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.2.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>8.2.7</b> Implementation and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>8.3</b> Feature Interaction<span></span></a><ul>
<li class="chapter" data-level="8.3.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>8.3.1</b> Feature Interaction?<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>8.3.2</b> Theory: Friedman’s H-statistic<span></span></a></li>
<li class="chapter" data-level="8.3.3" data-path="interaction.html"><a href="interaction.html#examples-2"><i class="fa fa-check"></i><b>8.3.3</b> Examples<span></span></a></li>
<li class="chapter" data-level="8.3.4" data-path="interaction.html"><a href="interaction.html#advantages-7"><i class="fa fa-check"></i><b>8.3.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.3.5" data-path="interaction.html"><a href="interaction.html#disadvantages-7"><i class="fa fa-check"></i><b>8.3.5</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.3.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>8.3.6</b> Implementations<span></span></a></li>
<li class="chapter" data-level="8.3.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>8.3.7</b> Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>8.4</b> Functional Decompositon<span></span></a><ul>
<li class="chapter" data-level="8.4.1" data-path="decomposition.html"><a href="decomposition.html#how-not-to-compute-the-components-i"><i class="fa fa-check"></i><b>8.4.1</b> How not to Compute the Components I<span></span></a></li>
<li class="chapter" data-level="8.4.2" data-path="decomposition.html"><a href="decomposition.html#functional-decomposition"><i class="fa fa-check"></i><b>8.4.2</b> Functional Decomposition<span></span></a></li>
<li class="chapter" data-level="8.4.3" data-path="decomposition.html"><a href="decomposition.html#how-not-to-compute-the-components-ii"><i class="fa fa-check"></i><b>8.4.3</b> How not to Compute the Components II<span></span></a></li>
<li class="chapter" data-level="8.4.4" data-path="decomposition.html"><a href="decomposition.html#functional-anova"><i class="fa fa-check"></i><b>8.4.4</b> Functional ANOVA<span></span></a></li>
<li class="chapter" data-level="8.4.5" data-path="decomposition.html"><a href="decomposition.html#generalized-functional-anova-for-dependent-features"><i class="fa fa-check"></i><b>8.4.5</b> Generalized Functional ANOVA for Dependent Features<span></span></a></li>
<li class="chapter" data-level="8.4.6" data-path="decomposition.html"><a href="decomposition.html#accumulated-local-effect-plots"><i class="fa fa-check"></i><b>8.4.6</b> Accumulated Local Effect Plots<span></span></a></li>
<li class="chapter" data-level="8.4.7" data-path="decomposition.html"><a href="decomposition.html#statistical-regression-models"><i class="fa fa-check"></i><b>8.4.7</b> Statistical Regression Models<span></span></a></li>
<li class="chapter" data-level="8.4.8" data-path="decomposition.html"><a href="decomposition.html#bonus-partial-dependence-plot"><i class="fa fa-check"></i><b>8.4.8</b> Bonus: Partial Dependence Plot<span></span></a></li>
<li class="chapter" data-level="8.4.9" data-path="decomposition.html"><a href="decomposition.html#advantages-8"><i class="fa fa-check"></i><b>8.4.9</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.4.10" data-path="decomposition.html"><a href="decomposition.html#disadvantages-8"><i class="fa fa-check"></i><b>8.4.10</b> Disadvantages<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>8.5</b> Permutation Feature Importance<span></span></a><ul>
<li class="chapter" data-level="8.5.1" data-path="feature-importance.html"><a href="feature-importance.html#theory-3"><i class="fa fa-check"></i><b>8.5.1</b> Theory<span></span></a></li>
<li class="chapter" data-level="8.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>8.5.2</b> Should I Compute Importance on Training or Test Data?<span></span></a></li>
<li class="chapter" data-level="8.5.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>8.5.3</b> Example and Interpretation<span></span></a></li>
<li class="chapter" data-level="8.5.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-9"><i class="fa fa-check"></i><b>8.5.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.5.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-9"><i class="fa fa-check"></i><b>8.5.5</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.5.6" data-path="feature-importance.html"><a href="feature-importance.html#alternatives-1"><i class="fa fa-check"></i><b>8.5.6</b> Alternatives<span></span></a></li>
<li class="chapter" data-level="8.5.7" data-path="feature-importance.html"><a href="feature-importance.html#software-3"><i class="fa fa-check"></i><b>8.5.7</b> Software<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>8.6</b> Global Surrogate<span></span></a><ul>
<li class="chapter" data-level="8.6.1" data-path="global.html"><a href="global.html#theory-4"><i class="fa fa-check"></i><b>8.6.1</b> Theory<span></span></a></li>
<li class="chapter" data-level="8.6.2" data-path="global.html"><a href="global.html#example-3"><i class="fa fa-check"></i><b>8.6.2</b> Example<span></span></a></li>
<li class="chapter" data-level="8.6.3" data-path="global.html"><a href="global.html#advantages-10"><i class="fa fa-check"></i><b>8.6.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.6.4" data-path="global.html"><a href="global.html#disadvantages-10"><i class="fa fa-check"></i><b>8.6.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.6.5" data-path="global.html"><a href="global.html#software-4"><i class="fa fa-check"></i><b>8.6.5</b> Software<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>8.7</b> Prototypes and Criticisms<span></span></a><ul>
<li class="chapter" data-level="8.7.1" data-path="proto.html"><a href="proto.html#theory-5"><i class="fa fa-check"></i><b>8.7.1</b> Theory<span></span></a></li>
<li class="chapter" data-level="8.7.2" data-path="proto.html"><a href="proto.html#examples-3"><i class="fa fa-check"></i><b>8.7.2</b> Examples<span></span></a></li>
<li class="chapter" data-level="8.7.3" data-path="proto.html"><a href="proto.html#advantages-11"><i class="fa fa-check"></i><b>8.7.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="8.7.4" data-path="proto.html"><a href="proto.html#disadvantages-11"><i class="fa fa-check"></i><b>8.7.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="8.7.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>8.7.5</b> Code and Alternatives<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="local-methods.html"><a href="local-methods.html"><i class="fa fa-check"></i><b>9</b> 
  <!--Local Model-Agnostic Methods-->Modelden Bağımsız Yerel Metotlar<span></span></a><ul>
<li class="chapter" data-level="9.1" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>9.1</b> Individual Conditional Expectation (ICE)<span></span></a><ul>
<li class="chapter" data-level="9.1.1" data-path="ice.html"><a href="ice.html#examples-4"><i class="fa fa-check"></i><b>9.1.1</b> Examples<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="ice.html"><a href="ice.html#advantages-12"><i class="fa fa-check"></i><b>9.1.2</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.1.3" data-path="ice.html"><a href="ice.html#disadvantages-12"><i class="fa fa-check"></i><b>9.1.3</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.1.4" data-path="ice.html"><a href="ice.html#software-and-alternatives-2"><i class="fa fa-check"></i><b>9.1.4</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>9.2</b> Local Surrogate (LIME)<span></span></a><ul>
<li class="chapter" data-level="9.2.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>9.2.1</b> LIME for Tabular Data<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>9.2.2</b> LIME for Text<span></span></a></li>
<li class="chapter" data-level="9.2.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>9.2.3</b> LIME for Images<span></span></a></li>
<li class="chapter" data-level="9.2.4" data-path="lime.html"><a href="lime.html#advantages-13"><i class="fa fa-check"></i><b>9.2.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.2.5" data-path="lime.html"><a href="lime.html#disadvantages-13"><i class="fa fa-check"></i><b>9.2.5</b> Disadvantages<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>9.3</b> Counterfactual Explanations<span></span></a><ul>
<li class="chapter" data-level="9.3.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>9.3.1</b> Generating Counterfactual Explanations<span></span></a></li>
<li class="chapter" data-level="9.3.2" data-path="counterfactual.html"><a href="counterfactual.html#example-8"><i class="fa fa-check"></i><b>9.3.2</b> Example<span></span></a></li>
<li class="chapter" data-level="9.3.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-14"><i class="fa fa-check"></i><b>9.3.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.3.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-14"><i class="fa fa-check"></i><b>9.3.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.3.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>9.3.5</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>9.4</b> Scoped Rules (Anchors)<span></span></a><ul>
<li class="chapter" data-level="9.4.1" data-path="anchors.html"><a href="anchors.html#finding-anchors"><i class="fa fa-check"></i><b>9.4.1</b> Finding Anchors<span></span></a></li>
<li class="chapter" data-level="9.4.2" data-path="anchors.html"><a href="anchors.html#complexity-and-runtime"><i class="fa fa-check"></i><b>9.4.2</b> Complexity and Runtime<span></span></a></li>
<li class="chapter" data-level="9.4.3" data-path="anchors.html"><a href="anchors.html#tabular-data-example"><i class="fa fa-check"></i><b>9.4.3</b> Tabular Data Example<span></span></a></li>
<li class="chapter" data-level="9.4.4" data-path="anchors.html"><a href="anchors.html#advantages-15"><i class="fa fa-check"></i><b>9.4.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.4.5" data-path="anchors.html"><a href="anchors.html#disadvantages-15"><i class="fa fa-check"></i><b>9.4.5</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.4.6" data-path="anchors.html"><a href="anchors.html#software-and-alternatives-3"><i class="fa fa-check"></i><b>9.4.6</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>9.5</b> Shapley Values<span></span></a><ul>
<li class="chapter" data-level="9.5.1" data-path="shapley.html"><a href="shapley.html#general-idea"><i class="fa fa-check"></i><b>9.5.1</b> General Idea<span></span></a></li>
<li class="chapter" data-level="9.5.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>9.5.2</b> Examples and Interpretation<span></span></a></li>
<li class="chapter" data-level="9.5.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>9.5.3</b> The Shapley Value in Detail<span></span></a></li>
<li class="chapter" data-level="9.5.4" data-path="shapley.html"><a href="shapley.html#advantages-16"><i class="fa fa-check"></i><b>9.5.4</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.5.5" data-path="shapley.html"><a href="shapley.html#disadvantages-16"><i class="fa fa-check"></i><b>9.5.5</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.5.6" data-path="shapley.html"><a href="shapley.html#software-and-alternatives-4"><i class="fa fa-check"></i><b>9.5.6</b> Software and Alternatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>9.6</b> SHAP (SHapley Additive exPlanations)<span></span></a><ul>
<li class="chapter" data-level="9.6.1" data-path="shap.html"><a href="shap.html#definition"><i class="fa fa-check"></i><b>9.6.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="9.6.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>9.6.2</b> KernelSHAP<span></span></a></li>
<li class="chapter" data-level="9.6.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>9.6.3</b> TreeSHAP<span></span></a></li>
<li class="chapter" data-level="9.6.4" data-path="shap.html"><a href="shap.html#examples-5"><i class="fa fa-check"></i><b>9.6.4</b> Examples<span></span></a></li>
<li class="chapter" data-level="9.6.5" data-path="shap.html"><a href="shap.html#shap-feature-importance"><i class="fa fa-check"></i><b>9.6.5</b> SHAP Feature Importance<span></span></a></li>
<li class="chapter" data-level="9.6.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>9.6.6</b> SHAP Summary Plot<span></span></a></li>
<li class="chapter" data-level="9.6.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>9.6.7</b> SHAP Dependence Plot<span></span></a></li>
<li class="chapter" data-level="9.6.8" data-path="shap.html"><a href="shap.html#shap-interaction-values"><i class="fa fa-check"></i><b>9.6.8</b> SHAP Interaction Values<span></span></a></li>
<li class="chapter" data-level="9.6.9" data-path="shap.html"><a href="shap.html#clustering-shapley-values"><i class="fa fa-check"></i><b>9.6.9</b> Clustering Shapley Values<span></span></a></li>
<li class="chapter" data-level="9.6.10" data-path="shap.html"><a href="shap.html#advantages-17"><i class="fa fa-check"></i><b>9.6.10</b> Advantages<span></span></a></li>
<li class="chapter" data-level="9.6.11" data-path="shap.html"><a href="shap.html#disadvantages-17"><i class="fa fa-check"></i><b>9.6.11</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="9.6.12" data-path="shap.html"><a href="shap.html#software-5"><i class="fa fa-check"></i><b>9.6.12</b> Software<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>10</b> Neural Network Interpretation<span></span></a><ul>
<li class="chapter" data-level="10.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>10.1</b> Learned Features<span></span></a><ul>
<li class="chapter" data-level="10.1.1" data-path="cnn-features.html"><a href="cnn-features.html#feature-visualization"><i class="fa fa-check"></i><b>10.1.1</b> Feature Visualization<span></span></a></li>
<li class="chapter" data-level="10.1.2" data-path="cnn-features.html"><a href="cnn-features.html#network-dissection"><i class="fa fa-check"></i><b>10.1.2</b> Network Dissection<span></span></a></li>
<li class="chapter" data-level="10.1.3" data-path="cnn-features.html"><a href="cnn-features.html#advantages-18"><i class="fa fa-check"></i><b>10.1.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="10.1.4" data-path="cnn-features.html"><a href="cnn-features.html#disadvantages-18"><i class="fa fa-check"></i><b>10.1.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="10.1.5" data-path="cnn-features.html"><a href="cnn-features.html#software-and-further-material"><i class="fa fa-check"></i><b>10.1.5</b> Software and Further Material<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html"><i class="fa fa-check"></i><b>10.2</b> Pixel Attribution (Saliency Maps)<span></span></a><ul>
<li class="chapter" data-level="10.2.1" data-path="pixel-attribution.html"><a href="pixel-attribution.html#vanilla-gradient-saliency-maps"><i class="fa fa-check"></i><b>10.2.1</b> Vanilla Gradient (Saliency Maps)<span></span></a></li>
<li class="chapter" data-level="10.2.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html#deconvnet"><i class="fa fa-check"></i><b>10.2.2</b> DeconvNet<span></span></a></li>
<li class="chapter" data-level="10.2.3" data-path="pixel-attribution.html"><a href="pixel-attribution.html#grad-cam"><i class="fa fa-check"></i><b>10.2.3</b> Grad-CAM<span></span></a></li>
<li class="chapter" data-level="10.2.4" data-path="pixel-attribution.html"><a href="pixel-attribution.html#guided-grad-cam"><i class="fa fa-check"></i><b>10.2.4</b> Guided Grad-CAM<span></span></a></li>
<li class="chapter" data-level="10.2.5" data-path="pixel-attribution.html"><a href="pixel-attribution.html#smoothgrad"><i class="fa fa-check"></i><b>10.2.5</b> SmoothGrad<span></span></a></li>
<li class="chapter" data-level="10.2.6" data-path="pixel-attribution.html"><a href="pixel-attribution.html#examples-6"><i class="fa fa-check"></i><b>10.2.6</b> Examples<span></span></a></li>
<li class="chapter" data-level="10.2.7" data-path="pixel-attribution.html"><a href="pixel-attribution.html#advantages-19"><i class="fa fa-check"></i><b>10.2.7</b> Advantages<span></span></a></li>
<li class="chapter" data-level="10.2.8" data-path="pixel-attribution.html"><a href="pixel-attribution.html#disadvantages-19"><i class="fa fa-check"></i><b>10.2.8</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="10.2.9" data-path="pixel-attribution.html"><a href="pixel-attribution.html#software-6"><i class="fa fa-check"></i><b>10.2.9</b> Software<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html"><i class="fa fa-check"></i><b>10.3</b> Detecting Concepts<span></span></a><ul>
<li class="chapter" data-level="10.3.1" data-path="detecting-concepts.html"><a href="detecting-concepts.html#tcav-testing-with-concept-activation-vectors"><i class="fa fa-check"></i><b>10.3.1</b> TCAV: Testing with Concept Activation Vectors<span></span></a></li>
<li class="chapter" data-level="10.3.2" data-path="detecting-concepts.html"><a href="detecting-concepts.html#example-9"><i class="fa fa-check"></i><b>10.3.2</b> Example<span></span></a></li>
<li class="chapter" data-level="10.3.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html#advantages-20"><i class="fa fa-check"></i><b>10.3.3</b> Advantages<span></span></a></li>
<li class="chapter" data-level="10.3.4" data-path="detecting-concepts.html"><a href="detecting-concepts.html#disadvantages-20"><i class="fa fa-check"></i><b>10.3.4</b> Disadvantages<span></span></a></li>
<li class="chapter" data-level="10.3.5" data-path="detecting-concepts.html"><a href="detecting-concepts.html#bonus-other-concept-based-approaches"><i class="fa fa-check"></i><b>10.3.5</b> Bonus: Other Concept-based Approaches<span></span></a></li>
<li class="chapter" data-level="10.3.6" data-path="detecting-concepts.html"><a href="detecting-concepts.html#software-7"><i class="fa fa-check"></i><b>10.3.6</b> Software<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>10.4</b> Adversarial Examples<span></span></a><ul>
<li class="chapter" data-level="10.4.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>10.4.1</b> Methods and Examples<span></span></a></li>
<li class="chapter" data-level="10.4.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>10.4.2</b> The Cybersecurity Perspective<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>10.5</b> Influential Instances<span></span></a><ul>
<li class="chapter" data-level="10.5.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>10.5.1</b> Deletion Diagnostics<span></span></a></li>
<li class="chapter" data-level="10.5.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>10.5.2</b> Influence Functions<span></span></a></li>
<li class="chapter" data-level="10.5.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>10.5.3</b> Advantages of Identifying Influential Instances<span></span></a></li>
<li class="chapter" data-level="10.5.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>10.5.4</b> Disadvantages of Identifying Influential Instances<span></span></a></li>
<li class="chapter" data-level="10.5.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-5"><i class="fa fa-check"></i><b>10.5.5</b> Software and Alternatives<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>11</b> A Look into the Crystal Ball<span></span></a><ul>
<li class="chapter" data-level="11.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>11.1</b> The Future of Machine Learning<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>11.2</b> The Future of Interpretability<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>12</b> Contribute to the Book<span></span></a></li>
<li class="chapter" data-level="13" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>13</b> Citing this Book<span></span></a></li>
<li class="chapter" data-level="14" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>14</b> Translations<span></span></a></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements<span></span></a></li>
<li><a href="references.html#references">References<span></span></a><ul>
<li><a href="r-packages-used.html#r-packages-used">R Packages Used<span></span></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li> 
<li><a href="https://christophmolnar.com/impressum/" target="_blank">Impressum</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="limo" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.1</span> Lineer Regression<a href="limo.html#limo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bir lineer regression modeli hedefi, nitelik girdilerinin ağırlıklandırılmış toplamı olarak
  tahmin etmeye çalışır. Öğrenilen ilişkinin lineer olması yorumlanabilirliği kolaylaştırır.
  Lineer regression modelleri uzun süre istatistikçiler, bilgisayar bilimcileri ve
  sayısal problemlerle uğraşan diğer insanlar tarafından kullanılmıştır.</p>
<p>Lineer modeller, sürekli bir hedef olan y'nin x niteliklerine olan bağlılığını modellemek için
  kullanılabilir. Öğrenilen ilişkiler lineerdir ve tekil bir örnek için şöyle gösterilir:
</p>
  <p><span class="math display">\[y=\beta_{0}+\beta_{1}x_{1}+\ldots+\beta_{p}x_{p}+\epsilon\]</span></p>
<p>Bir veri tanesinin sonucu için yapılan tahmin, verinin sahip olduğu p tane niteliğin 
  ağırlıklandırılmış toplamıdır. Beta'lar (<span class="math inline">\(\beta_{j}\)</span>)
  öğrenilen ağırlıkları ya da katsayıları ifade eder. Toplamdaki ilk ağırlığa
  (<span class="math inline">\(\beta_0\)</span>) intercept denir ve herhangi bir nitelikle
  çarpılmaz. Epsilon (<span class="math inline">\(\epsilon\)</span>) parametreler
  öğrendiğimiz halde hala yaptığımız hata miktarı, yani tahmin ve asıl sonuç arasındaki
  farktır. Bu hataların Gaussian bir dağılımı takip ettiği varsayılır, yani hem negatif
  hem de pozitif yönde ve çok sayıda küçük - az sayıda büyük hata yaparız.</p> 
<p>Optimal ağırlıkları hesaplamak için çeşitli metotlar kullanılabilir. En küçük kareler
  metodu genellikle hesaplanan ve asıl sonuçlar arasındaki farkların karelerinin toplamını
  minimize eden ağırlıkları bulmak için kullanılır: </p>
  <p><span class="math display">\[\hat{\boldsymbol{\beta}}=\arg\!\min_{\beta_0,\ldots,\beta_p}\sum_{i=1}^n\left(y^{(i)}-\left(\beta_0+\sum_{j=1}^p\beta_jx^{(i)}_{j}\right)\right)^{2}\]</span></p>
<p>Optimal ağırlıkların nasıl elde edileceğinin detayına girmeyeceğiz, ama eğer ilginizi
  çektiyse  “The Elements of Statistical Learning” (Friedman, Hastie and Tibshirani 2009)<a
  href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> 
  kitabındaki 3.2. bölümü ya da lineer regression modelleriyle ilgili diğer online
  kaynakları okuyabilirsiniz.</p>
<p>Lineer regression modellerinin en büyük avantajı lineerliktir: Tahmin etme sürecini
  basitleştirirler ve en önemlisi, bu lineer denklemlerin modüler seviyede yorumlanabilmesi
  (ağırlıkların yorumlanması) çok kolaydır. Lineer ve benzer modellerin, sosyoloji,
  psikoloji ve diğer sayısal araştırma alanlarında sıkça başvuruluyor olmasının temel
  sebebi budur. Örneğin, tıpta, hastanın klinik sonucunu tahmin etmek tek önemli şey değildir,
  aynı cinsiyet, yaş ve diğer niteliklerde ilacın etkisini yorumlanabilir bir şekilde
  hesaplamak da önemlidir.</p>
<p>Ağırlıklar tahmini karar aralıklarıyla beraber gelir. Karar aralıkları, ağırlık tahmini
  için "doğru" ağırlığı belli bir kararlılıkla içeren aralıktır. Örneğin, 2 ağırlığı için
  %95 karar aralığı 1'den 3'e olabilir. Bu aralığın yorumlanması şöyle olur: Bu tahmini
  yeni veriler için 100 kez tekrarlasaydık, karar aralığı 100 durumdan 95'inde doğru ağırlığı
  içerirdi (lineer regression'ın eldeki veri için doğru model olması durumunda).</p>
<p>Modelin "doğru" model olup olmadığı, verideki ilişkilerin yapılan varsayımlarla tutarlı olup
  olmadığına bağlıdır. Bu varsayımlar lineerlik, normallik, homoskedastisite (sabit
  varyans), bağımsızlık,
  niteliklerin sabitliği ve çoklu doğrusal bağlılığın yokluğu.
<p><strong>Lineerlik</strong><br />
Lineer regression modeli, tahmini niteliklerin bir lineer kombinasyonu olmaya zorlar; bu
  hem en güçlü hem de en zayıf yanıdır. Lineerlik yorumlanabilirliği sağlar. Lineer etkileri
  hesaplamak ve açıklamak kolaydır. Toplama içerdiklerinden faktörler kolayca ayırt edilebilir.
  Eğer niteliklerin birbiriyle ilişkili olduklarından veya bir niteliğin hedef değişkenle
  lineer olmayan bir ilişkisi olduğundan şüpheleniyorsanız, etkileşim terimleri ekleyebilir
  veya regression spline'lar kullanabilirsiniz.</p>
<p><strong>Normallik</strong><br />
  Eldeki niteliklere göre hedef değişkenin normal dağılımda olduğu varsayılır. Eğer bu 
  varsayım ihlal edilirse, ağırlıklar için hesaplanan karar aralıkları geçersizdir.
<p><strong>Homoskedastisite</strong> (sabit varyans)<br />
  Hata terimlerinin varyansının tüm nitelik uzayı boyunca sabit olduğu varsayılır. Bir evin fiyatını evin büyüklüğünden (metrekare) tahmin etmeye çalıştığınızı düşünün.
  Evin büyüklüğünden bağımsız olarak tahmindeki hatanın sabit varyansa sahip olduğunu varsayan bir lineer model oluşturuyorsunuz. Bu varsayım pratikte genellikle
  geçersizdir. Ev örneğinde, tahmindeki hataların büyük evler için daha yüksek varyansa sahip olması mantıklıdır, çünkü fiyatlar daha yüksektir ve 
  fiyattaki dalgalanmalar daha olasıdır. Lineer modelinizdeki ortalama hatanın (tahmin ve gerçek fiyat arasındaki ortalama fark) 50,000 Euro olduğunu düşünün.
  Eğer homoskedastisite varsayarsanız, ortalama hata olan 50,000 Euro'nun hem 1 milyonluk bir ev hem de 40,000 Euro'luk bir ev için aynı
  olduğunu varsaymış olursunuz. Bu mantıksızdır çünkü negatif fiyatların olabileceği anlamına gelir.</p>
<p><strong>Bağımsızlık</strong><br />
  Tüm veri örneklerinin diğerlerinden bağımsız olduğu varsayılır. Eğer tekrar tekrar ölçüm yaparsanız, bir hasta için birden çok kan testi yapmak gibi,
  veri taneleri (gözlemler) bağımsız değildir. Bağımsız olmayan veriler için karışık etki modelleri veya GEE'ler gibi özel bir lineer regression modeline
  ihtiyacınız olur. Eğer "normal" lineer regression kullanırsanız, modelden yanlış sonuçlar elde edebilirsiniz.</p>
<p><strong>Niteliklerin sabitliği</strong><br />
  Girdi nitelikler "sabit" kabul edilirler. Buradaki sabit, niteliklerin istatiktiksel değişkenler olarak değil, eldeki sabitler olarak kabul edildiği anlamına gelir.
  Bu, niteliklerin ölçümünde hata olmayacağı anlamına gelen gerçek dışı bir varsayımdır. Ancak, bu varsayımı yapmazsanız, girdilerdeki hata paylarını dikkate alan
  çok karmaşık hata modelleri kullanmanız gerekir; genelde bu istenmeyen bir durumdur.</p>
<p><strong>Çoklu doğrusal bağlılığın yokluğu</strong><br />
  Ağırlıkların hesaplanmasını karıştıracağından, aralarında güçlü korelasyonlar olan niteliklerle çalışmak istemezsiniz. İki niteliğin arasında güçlü
  korelasyonun olduğu bir durumda ağırlıkları hesaplamak sorunlu hale gelir çünkü niteliklerin etkileri toplamsaldır ve etkilerin hangi niteliğe atfedileceği belirsiz hale gelir.</p>

  <div id="interpretation" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.1.1</span> Yorumlama<a href="limo.html#interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bir lineer regression modelindeki ağırlığın yorumlanması, o ağırlığa karşılık gelen niteliğin türüne bağlıdır.</p>
<ul>
<li>Sayısal nitelik: Sayısal niteliğin bir birim artması tahmin edilen sonucu niteliğin ağırlığı kadar değiştirir. Sayısal niteliklere örnek olarak
  bir evin büyüklüğü düşünülebilir.</li>
<li>İkili nitelik: Herhangi bir veri örneği için iki değerden birini alan niteliklerdir. "Evin bahçesi var" niteliği bu tür niteliklere örnek verilebilir.
  Değerlerden biri referans kategorisi olarak adlandırılır (bazı programlama dillerinde 0 ile kodlanır), "Bahçe yok" gibi. Niteliği referans kategorisinden diğerine
  değiştirmek hesaplanan sonucu niteliğin ağırlığı kadar değiştirir.</li>
<li>Birden çok kategoriye sahip kategorik nitelik: Sınırlı sayıda değer alabilen niteliklerdir. "Halı, laminat, parke" kategorilerine sahio "zemin türü" niteliği
  örnek verilebilir. Çok kategorili bir nitelikle başa çıkmanın bir yolu, her kategorinin kendi ikili sütununa sahip olduğu one-hot kodlamadır.
  L tane kategoriye sahip bir kategorik nitelik için sadece L-1 tane sütuna ihtiyacınız vardır, çünkü L tane sütun kullandığınızda L. sütun gereksiz bilgi
  içerir (örneğin, bir veri örneği için 1'den L-1'e tüm sütunlar 0 değerine sahipse, bu niteliğin o veri örneği için değerinin L kategorisi olduğu anlaşılır).
  Her bir kategorinin yorumlanması, ikili niteliklerin yorumlanmasıyla aynıdır. R gibi bazı programlama dilleri, kategorik nitelikleri farklı şekillerde
  kodlamanıza olana sağlar; <a href="limo.html#cat-code">bunu bölümün ilerleyen kısımlarında inceleyeceğiz</a>.</li>
<li>Intercept <span class="math inline">\(\beta_0\)</span>:
  Intercept, tüm veriler için 1 değerini alan "sabit niteliğin" ağırlığıdır. Çoğu program intercept'i hesaplamak için bu "1" niteliğini otomatik olarak ekler.
  Yorumlanması ise şu şekildedir: Tüm sayısal nitelikleri sıfır ve kategorik nitelikleri referans kategorisi olan bir veri örneği için modelin yapacağı tahmin
  intercept ağırlığıdır. Intercept'in yorumlanması genellikle önemsizdir çünkü tüm değerleri sıfır olan bir gözlem anlamsızdır. Nitelikler standardize edildiğinde
  (ortalama 0, standart sapma 1) yorumlama anlamlı olur: O zaman intercept değeri, tüm değerleri niteliklerin ortalama değeri olan veri için
  yapılan tahmini temsil eder.</li>
</ul>
    
<p>Bir lineer regression modelindeki nitelikler, aşağıdaki şablonlarla otomatik olarak yorumlanabilir.</p>
<p><strong>Sayısal bir niteliğin yorumlanması</strong></p>
<p>Diğer tüm niteliklerin değerleri sabit tutulduğunda, <span class="math inline">\(x_{k}\)</span> niteliğindeki bir birimlik bir artış, y için yapılan tahmini <span class="math inline">\(\beta_k\)</span> birim
  arttırır.</p>
<p><strong>Kategorik bir niteliğin yorumlanması</strong></p>
<p>Diğer tüm niteliklerin değerleri sabit tutulduğunda, <span class="math inline">\(x_{k}\)</span> niteliğini referans kategorisinden diğer kategoriye değiştirmek, y için yapılan tahmini <span class="math inline">\(\beta_{k}\)</span> birim
  arttırır.</p>

<p>Lineer modellerin yorumlanmasında bir diğer ölçü R-kare'dir. R-kare, hedef değişkenin toplam varyansının ne kadarının model tarafından açıklandığını
  ölçer. R-kare değeriniz ne kadar fazlaysa, modeliniz veriyi o kadar iyi açıklıyor demektir. Formülü şu şekildedir:</p>
<p><span class="math display">\[R^2=1-HKT/VKT\]</span></p>
<p>HKT, hata terimlerinin karelerinin toplamıdır:</p>
<p><span class="math display">\[HKT=\sum_{i=1}^n(y^{(i)}-\hat{y}^{(i)})^2\]</span></p>
<p>VKT, verideki varyansın kareleri toplamıdır:</p>
<p><span class="math display">\[VKT=\sum_{i=1}^n(y^{(i)}-\bar{y})^2\]</span></p>
    
<p>HKT, modeli oluşturduktan sonra geride kalan varyansı temsil eder; gerçek ve tahmini değerlerin farklarının kareleriyle hesaplanır. VKT, hedef değişkenin
  toplam varyansıdır. R-kare, elinizdeki varyansın ne kadarının oluşturduğunuz lineer model tarafından açıklanabileceğini söyler. R-kare genellikle 0 ve 1 arasında;
  veriyi hiç açıklamayan modeller için 0, verideki tüm varyansı açıklayabilen modeller için 1 olmak üzere değer alır. R-kare'nin herhangi bir matematiksel
  hata olmadan negatif bir değer alması da mümkündür; bu durum HKT VKT'den büyük olduğunda, yani model verideki trendi açıklamadığında ve hedefin ortalamasını
  tahmin olarak kullanmaktan daha kötü sonuç verdiğinde gözlemlenir.</p>
<p>Bu haliyle formülde bir sıkıntı vardır çünkü R-kare değeri, modeldeki nitelik sayısıyla beraber, bu nitelikler hedef hakkında hiçbir bilgi
  vermeyen nitelikler de olsa artar. Bu yüzden modelde kullanılan nitelik sayısını da hesaba katan, ayarlanmış R-kare değerini kullanmak daha uygundur: 
    p niteliklerin sayısı ve n veri sayısı olmak üzere</p>
<p><span class="math display">\[\bar{R}^2=1-(1-R^2)\frac{n-1}{n-p-1}\]</span></p>
    
<p>Çok düşük (ayarlanmış) R-kare değerine sahip modelleri yorumlamak anlamsızdır, çünkü bu modeller varyansın büyük çoğunluğunu açıklamaz. Bu tür bir
  modelin ağırlıklarını yorumlamak anlamsız olacaktır.</p>
<p><strong>Niteliklerin Önemi</strong></p>
<p>Bir lineer regression modelindeki bir niteliğin önemi, niteliğin t-istatistiğinin mutlak değeriyle ölçülebilir. t-istatistiği, hesaplanan
  ağırlığın standart hatayla ölçeklendirilmiş halidir.</p>
<p><span class="math display">\[t_{\hat{\beta}_j}=\frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}\]</span></p>
<p>Bu formülün bize ne söylediğini inceleyelim: Bir niteliğin önemi ağırlığı arttıkça artar; bu gayet mantıklı. Hesaplanan ağırlığın varyansı arttıkça
  (ki bu doğru değerden gittikçe daha az emin olduğumuz anlamına gelir) niteliğin önemi azalır; bu da gayet mantıklıdır.</p>
</div>
<div id="example" class="section level3 hasAnchor">
  
<h3><span class="header-section-number">5.1.2</span> Örnek<a href="limo.html#example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bu örnekte hava durumu ve takvim bilgilerini kullanarak belli bir günde <a href="bike-data.html#bike-data">kiralanan bisiklet sayısını</a> lineer regression
  modeliyle tahmin etmeye çalışacağız. Yorumlama için, hesaplanan regression ağırlıklarını kullanacağız. Nitelikler sayısal ve kategorik niteliklerden oluşuyor.
  Tabloda her bir nitelik için hesaplanan ağırlık, hesaplamanın standart hatası (SH) ve t-istatistiğinin mutlak değeri yer alıyor (|t|).</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Ağırlık
</th>
<th style="text-align:right;">
SH
</th>
<th style="text-align:right;">
|t|
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
2399.4
</td>
<td style="text-align:right;">
238.3
</td>
<td style="text-align:right;">
10.1
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonSPRING
</td>
<td style="text-align:right;">
899.3
</td>
<td style="text-align:right;">
122.3
</td>
<td style="text-align:right;">
7.4
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonSUMMER
</td>
<td style="text-align:right;">
138.2
</td>
<td style="text-align:right;">
161.7
</td>
<td style="text-align:right;">
0.9
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonFALL
</td>
<td style="text-align:right;">
425.6
</td>
<td style="text-align:right;">
110.8
</td>
<td style="text-align:right;">
3.8
</td>
</tr>
<tr>
<td style="text-align:left;">
holidayHOLIDAY
</td>
<td style="text-align:right;">
-686.1
</td>
<td style="text-align:right;">
203.3
</td>
<td style="text-align:right;">
3.4
</td>
</tr>
<tr>
<td style="text-align:left;">
workingdayWORKING DAY
</td>
<td style="text-align:right;">
124.9
</td>
<td style="text-align:right;">
73.3
</td>
<td style="text-align:right;">
1.7
</td>
</tr>
<tr>
<td style="text-align:left;">
weathersitMISTY
</td>
<td style="text-align:right;">
-379.4
</td>
<td style="text-align:right;">
87.6
</td>
<td style="text-align:right;">
4.3
</td>
</tr>
<tr>
<td style="text-align:left;">
weathersitRAIN/SNOW/STORM
</td>
<td style="text-align:right;">
-1901.5
</td>
<td style="text-align:right;">
223.6
</td>
<td style="text-align:right;">
8.5
</td>
</tr>
<tr>
<td style="text-align:left;">
temp
</td>
<td style="text-align:right;">
110.7
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
15.7
</td>
</tr>
<tr>
<td style="text-align:left;">
hum
</td>
<td style="text-align:right;">
-17.4
</td>
<td style="text-align:right;">
3.2
</td>
<td style="text-align:right;">
5.5
</td>
</tr>
<tr>
<td style="text-align:left;">
windspeed
</td>
<td style="text-align:right;">
-42.5
</td>
<td style="text-align:right;">
6.9
</td>
<td style="text-align:right;">
6.2
</td>
</tr>
<tr>
<td style="text-align:left;">
days_since_2011
</td>
<td style="text-align:right;">
4.9
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
28.5
</td>
</tr>
</tbody>
</table>
  <!--Sırasıyla: Intercept, ilkbahar, yaz, sonbahar, tatil, iş günü, (hava durumu) sisli, (hava durumu) yağışlı/fırtınalı/karlı, sıcaklık, nem, rüzgar hızı, 2011'den
itibaren geçen gün sayısı-->
<p>Sayısal niteliğin yorumlanması (sıcaklık): Diğer tüm nitelikler sabit tutulduğunda, sıcaklıktaki 1 derece Celsius artış,
  tahmini kiralanan bisiklet sayısını 110.7 arttırır.
<p>Kategorik niteliğin yorumlanması (hava durumu):
  Interpretation of a categorical feature (“weathersit”): Diğer niteliklerin değişmediği varsayıldığında, havanın yağışlı/fırtınalı/karlı olması
  tahmini kiralanan bisiklet sayısını -1905.5 düşürür. Hava sisli olduğunda kiralanan bisiklet sayısı hava iyi olduğunda kiralanan bisiklet sayısından
  -379.4 daha azdır.</p>
<p>Lineer regression modellerinin doğası gereği, yapılan her yorum "diğer tüm nitelikler sabitken" dipnotunu beraberinde getirir.
  Tahmini değer niteliklerin lineer kombinasyonudur. Elde edilen lineer denklem, nitelik/hedef uzayında bir hiperdüzlem belirtir (bir niteliğe sahip bir
  model için basit bir doğru gibi). Ağırlıklar, bu hiperdüzlemin belli yönlerdeki eğimleridir (gradyan). Bu tür modellerin iyi bir yanı, toplamsallıklarının
  her niteliğin yorumlanmasını diğerlerinden bağımsız kılmasıdır; denklemde niteliklerin etkileri (ağırlık çarpı niteliğin değeri) bir toplamla bir araya getirildiğinden
  bu böyledir. Bunun kötü yanı, yorumlamaların, niteliklerin ortak dağılımlarını görmezden gelmesidir. Bir niteliğin artıp diğerinin sabit kalması gerçekçi olmayan
  ya da çok düşük ihtimalle karşılaşabileceğimiz veri örneklerine denk gelir, örneğin evdeki oda sayısının artması evin büyüklüğü artmadan mümkün değildir.</p>
</div>
<div id="visual-interpretation" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.1.3</span> Görsel Yorumlama<a href="limo.html#visual-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Lineer regression modellerini kolayca anlaşılabilir kılan birçok görselleştirme metodu vardır.</p>
<div id="weight-plot" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.1.3.1</span> Ağırlık Grafiği<a href="limo.html#weight-plot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Ağırlık tablosundaki bilgiler (ağırlıklar ve varyans hesapları) bir ağırlık grafiğinde görselleştirilebilir. Aşağıdaki tablo
  yukarıda incelediğimiz lineer regression modelinin sonuçlarını gösterir.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linear-weights-plot"></span>
<img src="images/linear-weights-plot-1.png" alt="Weights are displayed as points and the 95\% confidence intervals as lines." width="\textwidth" />
<p class="caption">
ŞEKİL 5.1: Noktalar ağırlıkları, çizgiler %95 karar aralıklarını temsil eder. 
</p>
</div>
<p>Grafik, yağışlı/karlı/fırtınalı havanın tahmini kiralanan bisiklet sayısına güçlü bir şekilde negatif yönde etki ettiğini gösteriyor. İş günü niteliğinin
  ağırlığının sıfıra çok yakın ve sıfırın karar aralığının içinde olması, niteliğin istatistiksel anlamda önemsiz olduğunu söylüyor. Sıcaklık gibi
  bazı nitelikler için karar aralığı çok kısa ve hesaplar sıfıra yakın, ama buna rağmen niteliğin istatistiksel anlamda etkisi önemli. Burada ağırlık grafiğiyle
  ilgili problem, niteliklerin farklı ölçü birimlerinde olması. Hava durumu nitelikleri için hesaplanan ağırlık iyi bir hava ile yağışlı/karlı/fırtınalı bir
  hava arasındaki farkı temsil ederken, sıcaklık niteliğinin ağırlığı 1 derece Celsius artışın etkisini temsil eder. Ağırlıkları karşılaştırılabilir kılmanın
  yolu, lineer modeli hesaplamadan önce nitelikleri ölçeklemektir (ortalamayı 0, standart sapmayı 1 yapacak şekilde).</p>
</div>
<div id="effect-plot" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.1.3.2</span> Etki Grafiği<a href="limo.html#effect-plot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Ağırlıklar nitelik değerleriyle çarpıldığında daha anlamlı yorumlanabilirler. Ağırlıklar niteliklerin ölçeklerine bağlıdırlar, örneğin bir insanın
  boyunu ölçüyorsanız metre yerine santimetre kullanmanız bu niteliğe karşılık gelen ağırlığı değiştirir; ama etkisi aynıdır. Niteliğin verideki
  dağılımını da bilmeniz oldukça önemlidir, çünkü örneğin niteliğin düşük bir varyansa sahip olması, neredeyse her veri örneğinin bu nitelik için
  aynı etkiye sahip olduğu anlamına gelir. Etki grafiği, ağırlık ve nitelik kombinasyonunun yapılan tahmine ne kadar etki ettiğini anlamanızı sağlar.
  Etki grafiğini çizebilmek için etkileri hesaplamak gerekir: Etki, ağırlık çarpı niteliğin o veri tanesi için aldığı değerdir.</p>
<p><span class="math display">\[\text{etki}_{j}^{(i)}=w_{j}x_{j}^{(i)}\]</span></p>
<p>Daha sonra etkiler kutu grafikleriyle çizilebilir. Kutu grafiklerindeki kutular, elinizdeki verinin yarısı için etki aralığını içerir
  (%25'den %75'e etki çeyrekliği). Kutunun içindeki dik çizgi medyan etkisini, yani verilerin %50'sinin daha fazla etkiye sahip ve diğer
  %50'sinin daha az etkiye sahip olduğu değeri temsil eder. ÇAA çeyrekler arası aralık olmak üzere yatay çizgiler
  <span class="math inline">\(\pm1.5\text{ÇAA}/\sqrt{n}\)</span> arasındadır (ÇAA, %75 çeyreklik ile %25 çeyreklik arasındaki farktır).
  Noktalar outlier değerleri temsil eder. Kategorik niteliklerin etkileri -her kategorinin kendisine özel satıra sahip olduğu
  ağırlık grafiğinin aksine- tek bir kutu grafiğinde özetlenebilir.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linear-effects"></span>
<img src="images/linear-effects-1.png" alt="The feature effect plot shows the distribution of effects (= feature value times feature weight) across the data per feature." width="\textwidth" />
<p class="caption">
ŞEKİL 5.2: Etki grafiği, her nitelik için veri boyunca etkilerin (= niteliğin değeri çarpı ağırlığı) dağılımını gösterir. 
</p>
</div>
<p>Tahmin edilen kiralık bisiklet sayısına en büyük katkıyı sıcaklık ve hedef değişkenin trendini içeren geçen gün sayısı niteliği sağlıyor. Sıcaklık özelliğinin
  etkisi büyük bir aralık içinde değişiyor. Geçen gün sayısının etkisi sıfırdan büyük pozitif sayılara değişiyor; verideki ilk günün (01.01.2011) çok düşük
  bir trend etkisi var ve niteliğin ağırlığı pozitif (4.93). Bu, etkinin her geçen gün arttığını ve en yüksek değerini verideki son gün için aldığını gösteriyor 
  (31.12.2012).
  Negatif etkiye sahip ağırlıklarda pozitif etkiye sahip veri örnekleri, negatif nitelik değerine sahip olan veriler olacaktır. Örneğin, rüzgar hızının 
  yüksek negatif etkiye sahip olduğu günler (örnekler) rüzgar hızının yüksek olduğu günlerdir.</p>
</div>
</div>
<div id="explain-individual-predictions" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.1.4</span> Tekil Tahminlerin Açıklanması<a href="limo.html#explain-individual-predictions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>"Bir veri örneğinin nitelikleri, onun için yapılan tahmini ne kadar etkiledi?" sorusunu bu veri için etkileri hesaplayarak bulabiliriz. Tekil veri
  için etkilerin yorumlanması ancak diğer niteliklerin etkilerinin dağılımıyla karşılaştırıldığında anlamlıdır. Diyelim ki, lineer modelin verideki
  6. örnek için yaptığı tahmini açıklamak istiyoruz. Verinin nitelik değerleri şu şekilde:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:linear-effects-single-table">TABLE 5.1: </span>Feature values for instance 6
</caption>
<thead>
<tr>
<th style="text-align:left;">
Feature
</th>
<th style="text-align:left;">
Value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
season
</td>
<td style="text-align:left;">
WINTER
</td>
</tr>
<tr>
<td style="text-align:left;">
yr
</td>
<td style="text-align:left;">
2011
</td>
</tr>
<tr>
<td style="text-align:left;">
mnth
</td>
<td style="text-align:left;">
JAN
</td>
</tr>
<tr>
<td style="text-align:left;">
holiday
</td>
<td style="text-align:left;">
NO HOLIDAY
</td>
</tr>
<tr>
<td style="text-align:left;">
weekday
</td>
<td style="text-align:left;">
THU
</td>
</tr>
<tr>
<td style="text-align:left;">
workingday
</td>
<td style="text-align:left;">
WORKING DAY
</td>
</tr>
<tr>
<td style="text-align:left;">
weathersit
</td>
<td style="text-align:left;">
GOOD
</td>
</tr>
<tr>
<td style="text-align:left;">
temp
</td>
<td style="text-align:left;">
1.604356
</td>
</tr>
<tr>
<td style="text-align:left;">
hum
</td>
<td style="text-align:left;">
51.8261
</td>
</tr>
<tr>
<td style="text-align:left;">
windspeed
</td>
<td style="text-align:left;">
6.000868
</td>
</tr>
<tr>
<td style="text-align:left;">
cnt
</td>
<td style="text-align:left;">
1606
</td>
</tr>
<tr>
<td style="text-align:left;">
days_since_2011
</td>
<td style="text-align:left;">
5
</td>
</tr>
</tbody>
</table>
<p>Niteliklerin etkilerini hesaplamak için nitelik değerleri ile modeldeki ağırlıkları çarpmalıyız. "workingday" (iş günü) niteliğinin
  "WORKING DAY" değeri için etki 124.9 olarak bulunur. 1.6 derece Celsius sıcaklık için etki 177.6 olarak hesaplanır. Bu tekil etkileri
  etkilerin tüm verideki dağılımlarını gösteren etki grafiğinde çarpıyla göstermek, tekil etkileri etkilerin dağılımıyla karşılaştırmamıza
  olanak sağlar.</p>
  <div class="figure" style="text-align: center"><span style="display:block;" id="fig:linear-effects-single"></span>
<img src="images/linear-effects-single-1.png" alt="The effect plot for one instance shows the effect distribution and highlights the effects of the instance of interest." width="\textwidth" />
<p class="caption">
ŞEKİL 5.3: Tekil veri için etki grafiği, etki dağılımını verinin kendi etkileriyle beraber gösterir. 
</p>
</div>
<p>Tüm veri için yapılan tahminlerin ortalaması 4504 bisiklettir. Bu sayıyla karşılaştırıldığında 6. veri örneği için yapılan tahmin, 1571, 
  küçük bir sayıdır; etki grafiği bunun nedenin bize gösterebilir. Grafikte gözlemlenebileceği üzere, 6. veri örneği düşük sıcaklık etkisine sahiptir
  çünkü o günün sıcaklık değeri 2 derece olup verideki çoğu günün sıcaklığından düşüktür (ve modelde sıcaklığın ağırlığı pozitiftir). Aynı zamanda, geçen gün sayısı
  niteliğinin etkisi diğer verilere göre daha düşüktür çünkü bu veri örneği 2011'in 5. gününe denk gelir, burada da bu niteliğin ağırlığı pozitiftir.</p>
  </div>
<div id="cat-code" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.1.5</span> Kategorik Niteliklerin Şifrelenmesi<a href="limo.html#cat-code" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Kategorik nitelikleri şifrelemek için birkaç yöntem var ve ağırlıkların yorumlanması seçtiğiniz yönteme göre değişiklik gösterir. </p>
<p>Lineer regression modelleri için standart yöntem tedavi şifrelemesidir, ve çoğu durumda yeterli bir çözümdür. Farklı şifreleme
  yöntemleri, kategorik niteliğin sütunundaki değerleri tasarım matrisine yerleştirme şekillerinde ayrışır. Bu kısımda üç farklı şifreleme yöntemine bakacağız,
  ama tüm metotlar bunlarla sınırlı değil. İlk iki veri örneği için nitelik A kategorisi, üçüncü ve dördüncü için nitelik B kategorisi ve son ikisi
  için C kategorisi değerini alır.</p>
  
<p><strong>Tedavi şifrelemesi</strong></p>
<p>Tedavi şifrelemesinde ağırlıklar, referans kategorisi ve eldeki kategori arasında tahmin değerinde oluşan farktır. Modeldeki intercept terimi,
  diğer tüm nitelikler aynı kaldığında, referans kategorisinin ortalamasıdır. Tasarım matrisinin ilk sütunu intercept, yani tamamı 1'lerden oluşan bir sütundur.
  İkinci sütun verinin B kategorisinde olup olmadığını, üçüncü sütun ise C kategorisinde olup olmadığını belirtir. A kategorisi için bir sütuna
  ihtiyaç yoktur, çünkü bu durumda lineer denklem "overspecified" olur ve ağırlıklar için biricik çözüm bulunamaz.
  Verinin ne B ne de C kategorisinde olmadığını bilmek yeterlidir.</p>
<p>Nitelik matrisi: <span class="math display">\[\begin{pmatrix}1&amp;0&amp;0\\1&amp;0&amp;0\\1&amp;1&amp;0\\1&amp;1&amp;0\\1&amp;0&amp;1\\1&amp;0&amp;1\\\end{pmatrix}\]</span></p>

  <p><strong>Etki şifrelemesi</strong></p>
<p>Her kategori için ağırlık, diğer nitelikler sıfır veya referans değerini aldığında, o kategoriyle genel ortalama arasında tahmindeki farktır. İlk sütun
  intercept'i hesaplamak için kullanılır. Intercept'in ağırlığı olan <span class="math inline">\(\beta_{0}\)</span> genel ortalamadır ve ikinci sütunun ağırlığı 
  <span class="math inline">\(\beta_{1}\)</span>, genel ortalama ve B kategorisi arasındaki farktır. B kategorisinin toplam etkisi <span class="math inline">\(\beta_{0}+\beta_{1}\)</span> olur.
  C kategorisinin yorumlanması da benzer şekilde yapılabilir. Referans kategorisi olan A için <span class="math inline">\(-(\beta_{1}+\beta_{2})\)</span>
  genel ortalamayla aradaki fark ve <span class="math inline">\(\beta_{0}-(\beta_{1}+\beta_{2})\)</span> genel etkidir.
<p>Nitelik matrisi: <span class="math display">\[\begin{pmatrix}1&amp;-1&amp;-1\\1&amp;-1&amp;-1\\1&amp;1&amp;0\\1&amp;1&amp;0\\1&amp;0&amp;1\\1&amp;0&amp;1\\\end{pmatrix}\]</span></p>

  <p><strong>Kukla şifrelemesi</strong></p>
<p>Her kategori için <span class="math inline">\(\beta\)</span>, o kategori için hesaplanan ortalama y değeridir (diğer tüm niteliklerin
  değerlerinin sıfır veya referans değeri olduğu varsayımıyla). Burada lineer modelin ağırlıklarının biricik bir çözümünü bulabilmek adına
  intercept ihmal edilir. Bu çoklu doğrusal bağlılık problemi herhangi bir kategoriyi ihmal ederek de çözülebilir.</p>
<p>Nitelik matrisi: <span class="math display">\[\begin{pmatrix}1&amp;0&amp;0\\1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\\0&amp;0&amp;1\\\end{pmatrix}\]</span></p>
<p>Kategorik niteliklerin farklı şifrelenme yöntemlerini incelemek istiyorsanız, 
  <a href="http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/">bu sayfaya</a> ve 
  <a href="http://heidiseibold.github.io/page7/">şu blog yazısına</a> bakabilirsiniz.</p>
</div>
<div id="do-linear-models-create-good-explanations" class="section level3 hasAnchor">
  
<h3><span class="header-section-number">5.1.6</span> Lineer Modeller İyi Açıklamalar Meydana Getirir Mi?<a href="limo.html#do-linear-models-create-good-explanations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><a href="explanation.html#good-explanation">İnsan dostu açıklamalar</a> kısmında bahsettiğimiz iyi bir açıklamanın gereklilikleri incelendiğinde,
  lineer modellerin en iyi açıklamalar için uygun olmadıkları görülür. Karşıtlıklardan faydalanırlar ama referans alınan veri örneği tüm sayısal değerlerin
  sıfır ve tüm kategorik değerlerin referans kategorisinde olduğu bir örnek olduğundan yapay ve anlamsızdır ve gerçekte verinizin içinde bulunma
  olasılığı çok düşüktür; bir istina dışında: Eğer tüm sayısal niteliklerde değerler ortalama merkezli hale getirilirse (niteliğin değeri eksi ortalama) ve
  tüm kategorik nitelikler etki şifrelemesiyle şifrelenirse, referans örneği tüm niteliklerin ortalama değerini aldığı örnek olur. Bu da var olmayan bir 
  örnek olabilir, ama en azından karşılaşması daha olası ve daha anlamlıdır. Bu durumda hesaplanan nitelik etkileri, tahmine olan katkıyı bu "ortalam örneğe"
  bağıl olarak temsil eder. İyi bir açıklamanın diğer özelliği seçilebilirliktir ve lineer modellerde daha az nitelik kullanarak veya aralıklı lineer modeller
  kullanarak elde edilebilir. Ama normal halleriyle lineer modeller farklı açıklamalar için seçenekler sunan açıklamalar oluşturmaz. Lineer modeller, lineer denklem
  nitelikler ve hedef değişken arasındaki ilişki için uygun olduğu durumlarda gerçekçi açıklamalar oluştururlar. Lineer olmayan ilişkilerin ve birbiriyle etkileşimli
  niteliklerin sayısı arttıkça, lineer model daha kötü performans gösterecek ve açıklamalar daha az gerçekçi olacaktır. Lineerlik, açıklamaların daha genel ve 
  basit olmasını sağlar, ki bence modellerin bu yapısı insanların açıklamalarda lineer modeller kullanmasının temel sebebidir.</p>
</div>
<div id="sparse-linear" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.1.7</span> Aralıklı Lineer Modeller<a href="limo.html#sparse-linear" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Şimdiye kadar seçtiğim örnekler gayet hoş duruyordu değil mi? Gerçekte, elinizdeki niteliklerin sayısı birkaç tane değil yüzlerce hatta binlerce olabilir.
  Peki o zaman lineer regression modelleri işe yarar mı? Yorumlanabilirlikleri hızla düşecektir. Niteliklerin örneklerden daha fazla olduğu durumlarla bile
  karşılaşabilirsiniz, o zaman standart bir lineer modeli hesaplayamazsınız bile. Bu durumlarda bizi kurtaracak olan şey modele aralıklılık 
  (daha az nitelikle çalışabilmeyi) getirecek yöntemlerdir. </p> 
  
  <div id="lasso" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.1.7.1</span> Lasso<a href="limo.html#lasso" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Lasso, lineer modelleri aralıklı yapmak için otomatik ve elverişli bir yöntemdir. Açılımı "en küçük mutlak küçülme ve seçim operatörü"dür ve bir lineer regression
  modeline uygulandığında, nitelik seçimini ve ağırlıkların regularization'ını sağlar. Ağırlıkları optimize edeceğimiz şu minimizasyon problemini düşünelim:</p>
<p><span class="math display">\[min_{\boldsymbol{\beta}}\left(\frac{1}{n}\sum_{i=1}^n(y^{(i)}-x_i^T\boldsymbol{\beta})^2\right)\]</span></p>
<p>Lasso bu optimizasyon problemine bir terim ekler:</p>
<p><span class="math display">\[min_{\boldsymbol{\beta}}\left(\frac{1}{n}\sum_{i=1}^n(y^{(i)}-x_{i}^T\boldsymbol{\beta})^2+\lambda||\boldsymbol{\beta}||_1\right)\]</span></p>
<p><span class="math inline">\(||\boldsymbol{\beta}||_1\)</span> terimi, nitelik vektörünün L1-normu, büyük ağırlık değerlerinin seçilmesini önler.
  L1-normu kullanıldığından çoğu ağırlık sıfır değerini alır, diğerleriyse küçülür. Lambda parametresi (<span class="math inline">\(\lambda\)</span>)
  regularization etkisinin gücünü kontrol eder ve genelde çapraz doğrulama ile ayarlanır. Özellikle lambda büyük olduğunda çoğu
  ağırlık sıfır değerini alır. Niteliklerin ağırlıkları lambda teriminin bir fonksiyonu olarak görselleştirilebilir. Her bir ağırlık aşağıdaki
  şekilde bir eğriyle gösterilmiştir:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lasso-path"></span>
<img src="images/lasso-path-1.png" alt="With increasing penalty of the weights, fewer and fewer features receive a non-zero weight estimate. These curves are also called regularization paths. The number above the plot is the number of non-zero weights." width="\textwidth" />
<p class="caption">
ŞEKİL 5.4: Lambda değeri arttıkça daha az nitelik sıfır olmayan bir ağırlık alır. Bu eğrilere regularization yolları da denir. Grafiğin üstündeki sayı sıfır olmayan ağırlık sayısıdır.
</p>
</div>
<p>Peki lambda değerini nasıl seçmeliyiz?
If you see the penalization term as a tuning parameter, then you can find the lambda that minimizes the model error with cross-validation.
You can also consider lambda as a parameter to control the interpretability of the model.
The larger the penalization, the fewer features are present in the model (because their weights are zero) and the better the model can be interpreted.</p>
<p><strong>Example with Lasso</strong></p>
<p>We will predict bicycle rentals using Lasso.
We set the number of features we want to have in the model beforehand.
Let us first set the number to 2 features:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Weight
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
seasonWINTER
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonSPRING
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonSUMMER
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonFALL
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
holidayHOLIDAY
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
workingdayWORKING DAY
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
weathersitMISTY
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
weathersitRAIN/SNOW/STORM
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
temp
</td>
<td style="text-align:right;">
52.33
</td>
</tr>
<tr>
<td style="text-align:left;">
hum
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
windspeed
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
days_since_2011
</td>
<td style="text-align:right;">
2.15
</td>
</tr>
</tbody>
</table>
<p>The first two features with non-zero weights in the Lasso path are temperature (“temp”) and the time trend (“days_since_2011”).</p>
<p>Now, let us select 5 features:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Weight
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
seasonWINTER
</td>
<td style="text-align:right;">
-389.99
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonSPRING
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonSUMMER
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
seasonFALL
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
holidayHOLIDAY
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
workingdayWORKING DAY
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
weathersitMISTY
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
weathersitRAIN/SNOW/STORM
</td>
<td style="text-align:right;">
-862.27
</td>
</tr>
<tr>
<td style="text-align:left;">
temp
</td>
<td style="text-align:right;">
85.58
</td>
</tr>
<tr>
<td style="text-align:left;">
hum
</td>
<td style="text-align:right;">
-3.04
</td>
</tr>
<tr>
<td style="text-align:left;">
windspeed
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;">
days_since_2011
</td>
<td style="text-align:right;">
3.82
</td>
</tr>
</tbody>
</table>
<p>Note that the weights for “temp” and “days_since_2011” differ from the model with two features.
The reason for this is that by decreasing lambda even features that are already “in” the model are penalized less and may get a larger absolute weight.
The interpretation of the Lasso weights corresponds to the interpretation of the weights in the linear regression model.
You only need to pay attention to whether the features are standardized or not, because this affects the weights.
In this example, the features were standardized by the software, but the weights were automatically transformed back for us to match the original feature scales.</p>
<p><strong>Other methods for sparsity in linear models</strong></p>
<p>A wide spectrum of methods can be used to reduce the number of features in a linear model.</p>
<p>Pre-processing methods:</p>
<ul>
<li>Manually selected features:
You can always use expert knowledge to select or discard some features.
The big drawback is that it cannot be automated and you need to have access to someone who understands the data.</li>
<li>Univariate selection:
An example is the correlation coefficient.
You only consider features that exceed a certain threshold of correlation between the feature and the target.
The disadvantage is that it only considers the features individually.
Some features might not show a correlation until the linear model has accounted for some other features.
Those ones you will miss with univariate selection methods.</li>
</ul>
<p>Step-wise methods:</p>
<ul>
<li>Forward selection:
Fit the linear model with one feature.
Do this with each feature.
Select the model that works best (e.g. highest R-squared).
Now again, for the remaining features, fit different versions of your model by adding each feature to your current best model.
Select the one that performs best.
Continue until some criterion is reached, such as the maximum number of features in the model.</li>
<li>Backward selection:
Similar to forward selection.
But instead of adding features, start with the model that contains all features and try out which feature you have to remove to get the highest performance increase.
Repeat this until some stopping criterion is reached.</li>
</ul>
<p>I recommend using Lasso, because it can be automated, considers all features simultaneously, and can be controlled via lambda.
It also works for the <a href="logistic.html#logistic">logistic regression model</a> for classification.</p>
</div>
</div>
<div id="advantages" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.1.8</span> Advantages<a href="limo.html#advantages" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The modeling of the predictions as a <strong>weighted sum</strong> makes it transparent how predictions are produced.
And with Lasso we can ensure that the number of features used remains small.</p>
<p>Many people use linear regression models.
This means that in many places it is <strong>accepted</strong> for predictive modeling and doing inference.
There is a <strong>high level of collective experience and expertise</strong>, including teaching materials on linear regression models and software implementations.
Linear regression can be found in R, Python, Java, Julia, Scala, Javascript, …</p>
<p>Mathematically, it is straightforward to estimate the weights and you have a <strong>guarantee to find optimal weights</strong> (given all assumptions of the linear regression model are met by the data).</p>
<p>Together with the weights you get confidence intervals, tests, and solid statistical theory.
There are also many extensions of the linear regression model (see <a href="extend-lm.html#extend-lm">chapter on GLM, GAM and more</a>).</p>
</div>
<div id="disadvantages" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.1.9</span> Disadvantages<a href="limo.html#disadvantages" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Linear regression models can only represent linear relationships, i.e. a weighted sum of the input features.
Each <strong>nonlinearity or interaction has to be hand-crafted</strong> and explicitly given to the model as an input feature.</p>
<p>Linear models are also often <strong>not that good regarding predictive performance</strong>, because the relationships that can be learned are so restricted and usually oversimplify how complex reality is.</p>
<p>The interpretation of a weight <strong>can be unintuitive</strong> because it depends on all other features.
A feature with high positive correlation with the outcome y and another feature might get a negative weight in the linear model, because, given the other correlated feature, it is negatively correlated with y in the high-dimensional space.
Completely correlated features make it even impossible to find a unique solution for the linear equation.
An example:
You have a model to predict the value of a house and have features like number of rooms and size of the house.
House size and number of rooms are highly correlated: the bigger a house is, the more rooms it has.
If you take both features into a linear model, it might happen, that the size of the house is the better predictor and gets a large positive weight.
The number of rooms might end up getting a negative weight, because, given that a house has the same size, increasing the number of rooms could make it less valuable or the linear equation becomes less stable, when the correlation is too strong.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="17">
<li id="fn17"><p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. “The elements of statistical learning”. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009).<a href="limo.html#fnref17" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/manuscript/04.2-interpretable-linear.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["interpretable-ml.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
